import os
import sys
import time
import traceback
import json
import requests
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS

# Add parent directory to path to import gemini_client
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# import google.generativeai as genai  # REMOVED: Legacy SDK
# from google import genai as genai_new  # REMOVED: New SDK
# from google.genai import types # REMOVED: New SDK types
import gemini_client # NEW: Pure REST Client
import re
from supabase import create_client, Client
from dotenv import load_dotenv
import io
import mimetypes

load_dotenv('.env.local')
# Remove static_folder config entirely to avoid any startup path issues
# We are serving files manually in home() and dashboard()
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Explicitly set template and static folders with absolute paths as requested
template_dir = os.path.join(BASE_DIR, 'public')
static_dir = os.path.join(BASE_DIR, 'public')
app = Flask(__name__, template_folder=template_dir, static_folder=static_dir)

@app.route('/ping')
def ping():
    return "pong", 200
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0 # Disable cache for development
CORS(app)

# File-based logging for debugging
def log_debug(message):
    try:
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_path = os.path.join(BASE_DIR, "debug.log")
        with open(log_path, "a") as f:
            f.write(f"[{timestamp}] {message}\n")
    except Exception as e:
        print(f"Logging failed: {e}", file=sys.stderr)

# Initialize log
# Initialize log
log_debug("Server started/reloaded")

# --- AGGRESSIVE LOGGING START ---
print(f"DEBUG: BASE_DIR is {BASE_DIR}", file=sys.stderr, flush=True)
print(f"DEBUG: template_dir is {template_dir}", file=sys.stderr, flush=True)

try:
    if os.path.exists(template_dir):
        print(f"DEBUG: Listing {template_dir}: {os.listdir(template_dir)}", file=sys.stderr, flush=True)
    else:
        print(f"DEBUG: template_dir does not exist!", file=sys.stderr, flush=True)
except Exception as e:
    print(f"DEBUG: Failed to list template_dir: {e}", file=sys.stderr, flush=True)

@app.before_request
def log_request_info():
    print(f"DEBUG: Request started: {request.method} {request.url}", file=sys.stderr, flush=True)
    # print(f"DEBUG: Headers: {request.headers}", file=sys.stderr, flush=True) # Uncomment if needed

@app.after_request
def log_response_info(response):
    print(f"DEBUG: Request finished: {response.status}", file=sys.stderr, flush=True)
    return response

@app.errorhandler(Exception)
def handle_exception(e):
    print(f"CRITICAL: Unhandled Exception: {str(e)}", file=sys.stderr, flush=True)
    traceback.print_exc()
    return jsonify({"error": "Internal Server Error", "details": str(e)}), 500
# --- AGGRESSIVE LOGGING END ---

@app.route('/api/get-debug-log', methods=['GET'])
def get_debug_log():
    try:
        log_path = os.path.join(BASE_DIR, "debug.log")
        if os.path.exists(log_path):
            with open(log_path, "r") as f:
                # Read last 50 lines
                lines = f.readlines()
                return jsonify({"logs": lines[-50:]}), 200
        return jsonify({"logs": ["Log file not found."]}), 200
    except Exception as e:
        return jsonify({"logs": [f"Error reading log: {str(e)}"]}), 200

import logging
try:
    log_path = os.path.join(BASE_DIR, 'backend.log')
    logging.basicConfig(filename=log_path, level=logging.INFO, 
                        format='%(asctime)s %(levelname)s: %(message)s')
    logger = logging.getLogger()
except Exception as e:
    print(f"Warning: Failed to setup file logging: {e}", file=sys.stderr)
    # Fallback to console logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger()

# Configure Gemini
# GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
# if not GEMINI_API_KEY:
#     # In production, this should ideally log an error or fail gracefully if the key is critical
#     pass 
# genai.configure(api_key=GEMINI_API_KEY) # REMOVED: Legacy SDK Config

# Configure Supabase
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# Add delay to allow connection pool to spin up (prevents startup crashes)
time.sleep(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None

@app.route('/')
def home():
    try:
        print("DEBUG: Entering home route", file=sys.stderr, flush=True)
        
        # Explicitly check for template existence
        template_path = os.path.join(template_dir, 'index.html')
        if not os.path.exists(template_path):
            error_msg = f"CRITICAL: Template not found at {template_path}"
            print(error_msg, file=sys.stderr, flush=True)
            return jsonify({"error": "Template not found", "path": template_path}), 500
            
        print(f"DEBUG: Serving template from {template_path}", file=sys.stderr, flush=True)
        return send_from_directory(template_dir, 'index.html')
        
    except Exception as e:
        print(f"CRITICAL ERROR in home route: {str(e)}", file=sys.stderr, flush=True)
        traceback.print_exc()
        return jsonify({"error": "Internal Server Error", "details": str(e)}), 500

@app.route('/health')
def health_check():
    print("DEBUG: Health check hit", file=sys.stderr, flush=True)
    return "OK", 200

@app.route('/debug-files')
def debug_files():
    try:
        files = os.listdir(app.static_folder)
        return jsonify({"static_folder": app.static_folder, "files": files})
    except Exception as e:
        return jsonify({"error": str(e), "static_folder": app.static_folder})


@app.route('/dashboard')
def dashboard():
    try:
        file_path = os.path.join(BASE_DIR, 'public', 'dashboard.html')
        if not os.path.exists(file_path):
            return f"Error: dashboard.html not found at {file_path}", 404
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        response = app.make_response(content)
        response.headers['Content-Type'] = 'text/html'
        response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, max-age=0'
        return response
    except Exception as e:
        return f"Server Error: {str(e)}", 500



@app.route('/api/test-ai', methods=['POST'])
def test_ai():
    if not os.environ.get("GEMINI_API_KEY"):
        return jsonify({"error": "GEMINI_API_KEY not found"}), 500

    try:
        data = request.get_json()
        topic = data.get('topic', 'SaaS Marketing') if data else 'SaaS Marketing'

        # Using the requested model which is confirmed to be available for this key
        # model = genai.GenerativeModel('gemini-2.5-flash')
        # response = model.generate_content(f"Write a short 1-sentence SEO strategy for '{topic}'.")
        
        generated_text = gemini_client.generate_content(
            prompt=f"Write a short 1-sentence SEO strategy for '{topic}'.",
            model_name="gemini-2.5-flash"
        )
        
        if not generated_text:
             return jsonify({"error": "Gemini generation failed"}), 500
             
        return jsonify({"strategy": generated_text.strip()})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/start-audit', methods=['POST'])
def start_audit():
    print("DEBUG: AUDIT FIX APPLIED - STARTING REQUEST")
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500

    try:
        data = request.get_json()
        page_id = data.get('page_id')
        
        if not page_id:
            return jsonify({"error": "page_id is required"}), 400
        
        # 1. Get the page
        page_res = supabase.table('pages').select('*').eq('id', page_id).execute()
        if not page_res.data:
            return jsonify({"error": "Page not found"}), 404
        
        page = page_res.data[0]
        target_url = page['url']
        
        print(f"DEBUG: Starting Tech Audit for {target_url}")
        
        # 2. Update status to PROCESSING
        supabase.table('pages').update({"audit_status": "Processing"}).eq('id', page_id).execute()
        
        # 3. Perform Tech Audit
        audit_data = {
            "status_code": None,
            "load_time_ms": 0,
            "title": None,
            "meta_description": None,
            "h1": None,
            "word_count": 0,
            "internal_links_count": 0,
            "broken_links": []
        }
        
        try:
            start_time = time.time()
            # Use a real browser User-Agent to avoid 403 Forbidden on sites like Apple
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            response = requests.get(target_url, headers=headers, timeout=15)
            audit_data["load_time_ms"] = int((time.time() - start_time) * 1000)
            audit_data["status_code"] = response.status_code
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Title
                audit_data["title"] = soup.title.string.strip() if soup.title else None
                
                # Meta Description
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc:
                    audit_data["meta_description"] = meta_desc.get('content', '').strip()
                
                # H1
                h1 = soup.find('h1')
                audit_data["h1"] = h1.get_text().strip() if h1 else None

                # Open Graph Tags
                og_title = soup.find('meta', attrs={'property': 'og:title'})
                audit_data["og_title"] = og_title.get('content', '').strip() if og_title else None
                
                og_desc = soup.find('meta', attrs={'property': 'og:description'})
                audit_data["og_description"] = og_desc.get('content', '').strip() if og_desc else None
                
                # Word Count (rough estimate)
                text = soup.get_text(separator=' ')
                words = [w for w in text.split() if len(w) > 2]
                audit_data["word_count"] = len(words)
                
                # Internal Links
                links = soup.find_all('a', href=True)
                audit_data["internal_links_count"] = len(links)

                # Canonical
                canonical = soup.find('link', attrs={'rel': 'canonical'})
                if canonical:
                    audit_data["canonical"] = canonical.get('href', '').strip()
                
                # Click Depth (Estimated based on URL path segments)
                import urllib.parse
                path = urllib.parse.urlparse(target_url).path
                # Root / is depth 0 or 1. Let's say root is 0.
                segments = [x for x in path.split('/') if x]
                audit_data["click_depth"] = len(segments)

                # --- On-Page Analysis ---
                score = 100
                checks = []
                
                # Title Analysis
                title = audit_data.get("title")
                if not title:
                    score -= 20
                    checks.append("Missing Title")
                    audit_data["title_length"] = 0
                else:
                    t_len = len(title)
                    audit_data["title_length"] = t_len
                    if t_len < 10: 
                        score -= 10
                        checks.append("Title too short")
                    elif t_len > 60:
                        score -= 10
                        checks.append("Title too long")

                # Meta Description Analysis
                desc = audit_data.get("meta_description")
                if not desc:
                    score -= 20
                    checks.append("Missing Meta Desc")
                    audit_data["description_length"] = 0
                else:
                    d_len = len(desc)
                    audit_data["description_length"] = d_len
                    if d_len < 50:
                        score -= 5
                        checks.append("Desc too short")
                    elif d_len > 160:
                        score -= 5
                        checks.append("Desc too long")

                # H1 Analysis
                h1 = audit_data.get("h1")
                if not h1:
                    score -= 20
                    checks.append("Missing H1")
                    audit_data["missing_h1"] = True
                else:
                    audit_data["missing_h1"] = False

                # OG Checks
                if not audit_data.get("og_title"):
                    checks.append("Missing OG Title")
                if not audit_data.get("og_description"):
                    checks.append("Missing OG Desc")
                
                # Image Alt Analysis
                images = soup.find_all('img')
                missing_alt = [img for img in images if not img.get('alt')]
                audit_data["missing_alt_count"] = len(missing_alt)
                
                if missing_alt:
                    score -= 10
                    checks.append(f"{len(missing_alt)} Images missing Alt")
                
                # --- Technical Issues Checks ---
                # Check for redirects using history
                if response.history:
                    audit_data["is_redirect"] = True
                    # You might want to store the initial status, but for now let's keep the final status for on-page checks
                    # or maybe store initial_status separately if needed.
                    # audit_data["status_code"] = response.history[0].status_code # Optional: if you want the 301
                else:
                    audit_data["is_redirect"] = 300 <= response.status_code < 400

                status = audit_data["status_code"]
                audit_data["is_4xx_code"] = 400 <= status < 500
                audit_data["is_5xx_code"] = 500 <= status < 600
                audit_data["high_loading_time"] = audit_data["load_time_ms"] > 2000
                
                # Advanced Checks
                audit_data["redirect_chain"] = len(response.history) > 1
                
                canonical = audit_data.get("canonical")
                if canonical and canonical != target_url:
                    audit_data["canonical_mismatch"] = True
                else:
                    audit_data["canonical_mismatch"] = False
                    
                audit_data["is_orphan_page"] = False # Placeholder: Requires full link graph
                
                # Final Checks
                audit_data["is_broken"] = status >= 400 or status == 0
                
                # Schema / Microdata Check
                has_json_ld = soup.find('script', type='application/ld+json') is not None
                has_microdata = soup.find(attrs={'itemscope': True}) is not None
                audit_data["has_schema"] = has_json_ld or has_microdata
                
                # Duplicate Checks (Query DB)
                try:
                    if title:
                        dup_title = supabase.table('pages').select('id', count='exact').eq('title', title).neq('id', page_id).execute()
                        audit_data["duplicate_title"] = dup_title.count > 0
                    else:
                        audit_data["duplicate_title"] = False
                        
                    if desc:
                        dup_desc = supabase.table('pages').select('id', count='exact').eq('meta_description', desc).neq('id', page_id).execute()
                        audit_data["duplicate_desc"] = dup_desc.count > 0
                    else:
                        audit_data["duplicate_desc"] = False
                except Exception as e:
                    print(f"Duplicate Check Error: {e}")
                    audit_data["duplicate_title"] = False
                    audit_data["duplicate_desc"] = False

                audit_data["onpage_score"] = max(0, score)
                audit_data["checks"] = checks

            else:
                print(f"Audit Failed: Status {response.status_code}")
                audit_data["error"] = f"HTTP {response.status_code}"
                audit_data["onpage_score"] = 0
                
        except Exception as e:
            print(f"Audit Error: {e}")
            audit_data["error"] = str(e)
            audit_data["status_code"] = 0 # Indicate failure
    
    # 4. Save Results (Merge with existing)
        current_tech_data = page.get('tech_audit_data') or {}
        current_tech_data.update(audit_data)
        
        update_payload = {
            "audit_status": "Analyzed",
            "tech_audit_data": current_tech_data,
            # Also update core fields if found
            "title": audit_data.get("title") or page.get("title"),
            "meta_description": audit_data.get("meta_description") or page.get("meta_description"),
            "h1": audit_data.get("h1") or page.get("h1")
        }
        
        print(f"DEBUG: Updating DB for page {page_id}")
        print(f"DEBUG: Payload: {json.dumps(update_payload, default=str)[:500]}...") # Print first 500 chars
        
        res = supabase.table('pages').update(update_payload).eq('id', page_id).execute()
        print(f"DEBUG: DB Update Result: {res}")
        
        return jsonify({
            "message": "Tech audit completed",
            "data": audit_data
        })

    except Exception as e:
        print(f"ERROR in start_audit: {str(e)}")
        import traceback
        traceback.print_exc()
        supabase.table('pages').update({"audit_status": "Failed"}).eq('id', page_id).execute()
        return jsonify({"error": str(e)}), 500

        return jsonify({"error": str(e)}), 500

@app.route('/api/analyze-speed', methods=['POST'])
def analyze_speed():
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        data = request.get_json()
        page_id = data.get('page_id')
        strategy = data.get('strategy', 'mobile') # mobile or desktop
        
        if not page_id: return jsonify({"error": "page_id required"}), 400
        
        # Fetch Page
        page_res = supabase.table('pages').select('url, tech_audit_data').eq('id', page_id).single().execute()
        if not page_res.data: return jsonify({"error": "Page not found"}), 404
        page = page_res.data
        url = page['url']
        
        print(f"Running PageSpeed ({strategy}) for {url}...")
        
        # Call Google PageSpeed Insights API
        psi_key = os.environ.get("PAGESPEED_API_KEY")
        psi_url = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&strategy={strategy}"
        if psi_key:
            psi_url += f"&key={psi_key}"
            
        psi_res = requests.get(psi_url, timeout=60)
        
        if psi_res.status_code != 200:
            return jsonify({"error": f"PSI API Failed: {psi_res.text}"}), 400
            
        psi_data = psi_res.json()
        
        # Extract Metrics
        lighthouse = psi_data.get('lighthouseResult', {})
        audits = lighthouse.get('audits', {})
        categories = lighthouse.get('categories', {})
        
        score = categories.get('performance', {}).get('score', 0) * 100
        fcp = audits.get('first-contentful-paint', {}).get('displayValue')
        lcp = audits.get('largest-contentful-paint', {}).get('displayValue')
        cls = audits.get('cumulative-layout-shift', {}).get('displayValue')
        tti = audits.get('interactive', {}).get('displayValue')
        
        # Update DB
        current_data = page.get('tech_audit_data') or {}
        speed_data = current_data.get('speed', {})
        speed_data[strategy] = {
            "score": score,
            "fcp": fcp,
            "lcp": lcp,
            "cls": cls,
            "tti": tti,
            "last_run": int(time.time())
        }
        current_data['speed'] = speed_data
        
        supabase.table('pages').update({"tech_audit_data": current_data}).eq('id', page_id).execute()
        
        return jsonify({
            "message": "Speed analysis complete",
            "data": speed_data[strategy]
        })
        
    except Exception as e:
        print(f"Speed Audit Error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/update-page-status', methods=['POST'])
def update_page_status():
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        data = request.get_json()
        page_id = data.get('page_id')
        updates = {}
        
        if 'funnel_stage' in data:
            updates['funnel_stage'] = data['funnel_stage']
            
        if 'page_type' in data:
            updates['page_type'] = data['page_type']
            
            # Auto-fetch title if classifying as Product and title is missing
            if data['page_type'] == 'Product':
                try:
                    # Get current page data
                    page_res = supabase.table('pages').select('url, tech_audit_data').eq('id', page_id).execute()
                    if page_res.data:
                        page = page_res.data[0]
                        tech_data = page.get('tech_audit_data') or {}
                        
                        if not tech_data.get('title') or tech_data.get('title') == 'Untitled Product':
                            print(f"Auto-fetching title for {page['url']}...")
                            try:
                                resp = requests.get(page['url'], headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                                if resp.status_code == 200:
                                    soup = BeautifulSoup(resp.content, 'html.parser')
                                    if soup.title and soup.title.string:
                                        raw_title = soup.title.string.strip()
                                        new_title = clean_title(raw_title)
                                        tech_data['title'] = new_title
                                        updates['tech_audit_data'] = tech_data
                                        print(f"Fetched title: {new_title}")
                            except Exception as scrape_err:
                                print(f"Scrape failed: {scrape_err}")
                except Exception as e:
                    print(f"Auto-fetch error: {e}")

        if 'approval_status' in data:
            updates['approval_status'] = data['approval_status']
            
        if not updates:
            return jsonify({"error": "No updates provided"}), 400
            
        supabase.table('pages').update(updates).eq('id', page_id).execute()
        return jsonify({"message": "Page updated successfully", "updates": updates})
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

import requests
from urllib.parse import urlparse

# ... (existing imports)

# Configure DataForSEO
DATAFORSEO_LOGIN = os.environ.get("DATAFORSEO_LOGIN")
DATAFORSEO_PASSWORD = os.environ.get("DATAFORSEO_PASSWORD")

def get_ranking_keywords(target_url):
    if not DATAFORSEO_LOGIN or not DATAFORSEO_PASSWORD:
        print("DataForSEO credentials missing.")
        return []

    try:
        # Clean URL to get domain (DataForSEO prefers domain without protocol)
        parsed = urlparse(target_url)
        domain = parsed.netloc if parsed.netloc else parsed.path
        if domain.startswith("www."):
            domain = domain[4:]
        
        # Normalize the target URL for comparison (remove protocol, www, trailing slash)
        normalized_target = target_url.lower().replace('https://', '').replace('http://', '').replace('www.', '').rstrip('/')
        
        print(f"DEBUG: Looking for keywords for normalized URL: {normalized_target}")

        url = "https://api.dataforseo.com/v3/dataforseo_labs/google/ranked_keywords/live"
        payload = [
            {
                "target": domain,
                "location_code": 2840, # US
                "language_code": "en",
                "filters": [
                    ["ranked_serp_element.serp_item.rank_absolute", ">=", 1],
                    "and",
                    ["ranked_serp_element.serp_item.rank_absolute", "<=", 10]
                ],
                "order_by": ["keyword_data.keyword_info.search_volume,desc"],
                "limit": 100  # Get more results to filter
            }
        ]
        headers = {
            'content-type': 'application/json'
        }

        response = requests.post(url, json=payload, auth=(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD), headers=headers)
        response.raise_for_status()
        data = response.json()

        page_keywords = []
        domain_keywords = []
        
        if data['tasks'] and data['tasks'][0]['result'] and data['tasks'][0]['result'][0]['items']:
            for item in data['tasks'][0]['result'][0]['items']:
                keyword = item['keyword_data']['keyword']
                volume = item['keyword_data']['keyword_info']['search_volume']
                
                # Get the ranking URL for this keyword
                ranking_url = item.get('ranked_serp_element', {}).get('serp_item', {}).get('url', '')
                # Normalize ranking URL the same way
                normalized_ranking = ranking_url.lower().replace('https://', '').replace('http://', '').replace('www.', '').rstrip('/')
                
                # Check if this keyword ranks for the specific page
                if normalized_ranking == normalized_target:
                    page_keywords.append(f"{keyword} (Vol: {volume})")
                    print(f"DEBUG: ✓ Page match: {keyword} ranks for {normalized_ranking}")
                elif normalized_ranking.startswith(domain):
                    domain_keywords.append(f"{keyword} (Vol: {volume})")
        
        # If we found page-specific keywords, return those (up to 5)
        if page_keywords:
            print(f"DEBUG: ✓ Found {len(page_keywords)} page-specific keywords for {target_url}")
            return page_keywords[:5]
        
        # Otherwise, return only 3 domain keywords as fallback
        if domain_keywords:
            print(f"DEBUG: ⚠ No page-specific keywords found. Using 3 domain-level keywords as fallback")
            return domain_keywords[:3]
        
        print(f"DEBUG: ✗ No keywords found at all for {target_url}")
        return []

    except Exception as e:
        print(f"DataForSEO Error: {e}")
        import traceback
        traceback.print_exc()
        return []

@app.route('/api/process-job', methods=['POST'])
def process_job():
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500

    try:
        # Step A: Fetch one pending job
        response = supabase.table('audit_results').select("*").eq('status', 'PENDING').limit(1).execute()
        
        if not response.data:
            return jsonify({"message": "No pending jobs"})
        
        job = response.data[0]
        job_id = job['id']
        target_url = job.get('url')
        if not target_url:
            target_url = 'example.com'
        
        # Step B: Lock (Update status to PROCESSING)
        supabase.table('audit_results').update({"status": "PROCESSING"}).eq('id', job_id).execute()
        
        # Step C: Work (Generate SEO audit)
        
        # 1. Get Keywords (Graceful degradation)
        keywords = get_ranking_keywords(target_url)
        keywords_str = ", ".join(keywords) if keywords else "No specific ranking keywords found."

        # 2. Generate Audit with Gemini
        # model = genai.GenerativeModel('gemini-2.5-flash')
        
        prompt = f"Analyze SEO for {target_url}. It currently ranks for these top keywords: {keywords_str}. Based on this, suggest 3 new content topics."
        
        audit_result = gemini_client.generate_content(
            prompt=prompt,
            model_name="gemini-2.5-flash"
        )
        
        if not audit_result:
            audit_result = "Audit generation failed."
        
        # Step D: Save (Update result and status to COMPLETED)
        supabase.table('audit_results').update({
            "status": "COMPLETED",
            "result": audit_result
        }).eq('id', job_id).execute()
        
        return jsonify({
            "id": job_id,
            "status": "COMPLETED",
            "result": audit_result
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/write-article', methods=['POST'])
def write_article():
    if not os.environ.get("GEMINI_API_KEY"):
        return jsonify({"error": "GEMINI_API_KEY not found"}), 500

    try:
        data = request.get_json()
        topic = data.get('topic')
        keywords = data.get('keywords', [])

        if not topic:
            return jsonify({"error": "Topic is required"}), 400

        # model = genai.GenerativeModel('gemini-2.5-flash')
        
        system_instruction = "You are an expert SEO content writer. Write a comprehensive, engaging 1,500-word blog post about the given topic. Use H2 and H3 headers. Format in Markdown. Include a catchy title."
        
        keywords_str = ', '.join(keywords) if keywords else 'relevant SEO keywords'
        full_prompt = f"{system_instruction}\n\nTopic: {topic}\nTarget Keywords: {keywords_str}"
        
        generated_text = gemini_client.generate_content(
            prompt=full_prompt,
            model_name="gemini-2.5-flash"
        )
        
        if not generated_text:
             return jsonify({"error": "Gemini generation failed"}), 500
             
        return jsonify({"content": generated_text.strip()})

    except Exception as e:
        return jsonify({"error": str(e)}), 500

from bs4 import BeautifulSoup

# ... (existing imports)





import subprocess

def fetch_with_curl(url, use_chrome_ua=True):
    """Fetch URL using system curl to bypass TLS fingerprinting blocks."""
    try:
        cmd = ['curl', '-L', '-s']
        
        if use_chrome_ua:
            cmd.extend([
                '-A', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                '-H', 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                '-H', 'Accept-Language: en-US,en;q=0.9',
            ])
            
        cmd.append(url)
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=20)
        
        # If failed with Chrome UA, retry without it (some sites like Akamai block fake UAs but allow curl)
        if use_chrome_ua and (result.returncode != 0 or not result.stdout or "Access Denied" in result.stdout):
            print(f"DEBUG: Chrome UA failed for {url}, retrying with default curl UA...")
            return fetch_with_curl(url, use_chrome_ua=False)
            
        if result.returncode == 0:
            return result.stdout
        else:
            print(f"DEBUG: curl failed with code {result.returncode}: {result.stderr}")
            return None
    except Exception as e:
        print(f"DEBUG: curl exception: {e}")
        return None

def crawl_sitemap(domain, project_id, max_pages=200):
    """Recursively crawl sitemaps with anti-bot headers"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1'
    }
    
    base_domain = domain.rstrip('/') if domain.startswith('http') else f"https://{domain.rstrip('/')}"
    sitemap_urls = []
    
    # 1. Try robots.txt first
    robots_url = f"{base_domain}/robots.txt"
    print(f"DEBUG: Fetching robots.txt: {robots_url}")
    try:
        robots_res = requests.get(robots_url, headers=headers, timeout=10)
        if robots_res.status_code == 200:
            for line in robots_res.text.splitlines():
                if line.lower().startswith('sitemap:'):
                    sitemap_url = line.split(':', 1)[1].strip()
                    sitemap_urls.append(sitemap_url)
            print(f"DEBUG: Found {len(sitemap_urls)} sitemaps in robots.txt")
    except Exception as e:
        print(f"DEBUG: Failed to fetch robots.txt: {e}")

    # 2. Fallback to common paths
    if not sitemap_urls:
        sitemap_urls = [
            f"{base_domain}/sitemap.xml",
            f"{base_domain}/sitemap_index.xml",
            f"{base_domain}/sitemap.php"
        ]

    pages = []
    
    # 3. Process each sitemap
    for sitemap_url in sitemap_urls:
        if len(pages) >= max_pages:
            break
        pages.extend(fetch_sitemap_urls(sitemap_url, project_id, headers, max_pages - len(pages)))
    
    return pages

def clean_title(title):
    """Clean up product titles by removing common e-commerce patterns."""
    if not title: return "Untitled Product"
    
    # Remove "Buy " from start (case insensitive)
    import re
    title = re.sub(r'^buy\s+', '', title, flags=re.IGNORECASE)
    
    # Remove " Online" from end (case insensitive)
    title = re.sub(r'\s+online$', '', title, flags=re.IGNORECASE)
    
    # Remove " - [Brand]" or " | [Brand]" suffix
    # Heuristic: split by " - " or " | " and take the first part if it's long enough
    separators = [" - ", " | ", " – "]
    for sep in separators:
        if sep in title:
            parts = title.split(sep)
            if len(parts[0]) > 3: # Avoid cutting too much if title is short
                title = parts[0]
                break
                
    return title.strip()

def fetch_sitemap_urls(sitemap_url, project_id, headers, max_urls):
    """Fetch URLs from a sitemap, recursively handling sitemap indexes"""
    print(f"DEBUG: Fetching sitemap: {sitemap_url}")
    pages = []
    
    try:
        # Use curl to bypass TLS fingerprinting
        content = fetch_with_curl(sitemap_url)
        
        if not content:
            print(f"DEBUG: Failed to fetch {sitemap_url} (curl returned empty)")
            return pages

        # Try parsing with XML, fallback to HTML parser if needed
        try:
            soup = BeautifulSoup(content, 'xml')
        except:
            soup = BeautifulSoup(content, 'html.parser')
        
        # Check if this is a sitemap index (contains <sitemap> tags)
        sitemap_tags = soup.find_all('sitemap')
        
        if sitemap_tags:
            print(f"DEBUG: Found sitemap index with {len(sitemap_tags)} child sitemaps")
            # Recursively fetch first 5 child sitemaps (increased from 2)
            for i, sitemap_tag in enumerate(sitemap_tags[:5]):
                if len(pages) >= max_urls:
                    break
                    
                loc = sitemap_tag.find('loc')
                if loc:
                    child_url = loc.text.strip()
                    print(f"DEBUG: Recursively fetching child sitemap {i+1}: {child_url}")
                    child_pages = fetch_sitemap_urls(child_url, project_id, headers, max_urls - len(pages))
                    pages.extend(child_pages)
        else:
            # Regular sitemap with <url> tags
            url_tags = soup.find_all('url')
            print(f"DEBUG: Found {len(url_tags)} URLs in sitemap")
            
            for tag in url_tags:
                if len(pages) >= max_urls:
                    break
                    
                loc = tag.find('loc')
                if loc and loc.text.strip():
                    url = loc.text.strip()
                    
                    # Skip title scraping for speed. 
                    # User can run "Perform Audit" to get details.
                    title = "Pending Scan"

                    pages.append({
                        'project_id': project_id,
                        'url': url,
                        'status': 'DISCOVERED',
                        'tech_audit_data': {'title': title} 
                    })
    
    except Exception as e:
        print(f"DEBUG: Error fetching sitemap {sitemap_url}: {e}")
    
    return pages


# Helper function to upload to Supabase Storage
def upload_to_supabase(file_data, filename, bucket_name='photoshoots'):
    """
    Uploads file data (bytes) to Supabase Storage and returns the public URL.
    """
    import mimetypes
    try:
        # Guess mime type
        mime_type, _ = mimetypes.guess_type(filename)
        if not mime_type:
            mime_type = 'application/octet-stream'
            
        # Upload
        res = supabase.storage.from_(bucket_name).upload(
            path=filename,
            file=file_data,
            file_options={"content-type": mime_type, "upsert": "true"}
        )
        
        # Get Public URL
        public_url = supabase.storage.from_(bucket_name).get_public_url(filename)
        return public_url
    except Exception as e:
        print(f"Supabase Upload Error: {e}")
        raise e

# Helper to load image from URL or Path
def load_image_data(source):
    """
    Loads image data from a URL (starts with http) or local path.
    Returns PIL Image object.
    """
    import PIL.Image
    import io
    import os
    if source.startswith('http'):
        print(f"Downloading image from URL: {source}")
        resp = requests.get(source)
        resp.raise_for_status()
        return PIL.Image.open(io.BytesIO(resp.content))
    else:
        # Assume local path relative to public
        # Handle cases where source might be just filename or /uploads/filename
        clean_path = source.lstrip('/')
        local_path = os.path.join(os.getcwd(), 'public', clean_path)
        print(f"Loading image from local path: {local_path}")
        if os.path.exists(local_path):
            return PIL.Image.open(local_path)
        else:
            # Try absolute path just in case
            if os.path.exists(source):
                return PIL.Image.open(source)
            raise Exception(f"Image not found at {source} or {local_path}")



@app.route('/api/get-projects', methods=['GET'])
def get_projects():
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        # Fetch projects
        projects_res = supabase.table('projects').select('*').order('created_at', desc=True).execute()
        projects = projects_res.data if projects_res.data else []
        
        if not projects:
            return jsonify({"projects": []})
        
        # Fetch profiles for these projects
        try:
            profiles_res = supabase.table('business_profiles').select('*').execute()
            profiles_data = profiles_res.data if profiles_res.data else []
            profiles_map = {p['project_id']: p for p in profiles_data}
        except Exception as e:
            print(f"Error fetching profiles: {e}")
            profiles_map = {}
        
        
        # Calculate counts per project (more reliable than bulk fetch with limit)
        from collections import defaultdict
        counts = defaultdict(int)
        classified_counts = defaultdict(int)
        
        
        # Calculate counts per project (OPTIMIZED: Batched fetch + In-memory aggregation)
        # This avoids N+1 query problem which causes slow loading
        from collections import defaultdict
        counts = defaultdict(int)
        classified_counts = defaultdict(int)
        
        try:
            all_pages = []
            has_more = True
            offset = 0
            limit = 5000 # Fetch in large chunks to minimize requests
            
            while has_more:
                # Fetch just the columns we need
                res = supabase.table('pages').select('project_id, page_type').range(offset, offset + limit - 1).execute()
                batch = res.data if res.data else []
                
                all_pages.extend(batch)
                
                if len(batch) < limit:
                    has_more = False
                offset += limit
                
            # Aggregate counts
            for page in all_pages:
                pid = page.get('project_id')
                if pid:
                    counts[pid] += 1
                    pt = page.get('page_type')
                    if pt and pt.lower() != 'unclassified':
                        classified_counts[pid] += 1
                        
        except Exception as e:
            print(f"Error fetching pages for counts: {e}")
            # Fallback to 0 counts if fetch fails, don't crash the whole endpoint

        # Merge and parse strategy plan
        final_projects = []
        for p in projects:
            try:
                profile = profiles_map.get(p['id'], {})
                
                # Parse Strategy Plan from Business Summary if present (WORKAROUND)
                summary = profile.get('business_summary') or ''
                strategy_plan = ''
                if '===STRATEGY_PLAN===' in summary:
                    try:
                        parts = summary.split('===STRATEGY_PLAN===')
                        summary = parts[0].strip()
                        if len(parts) > 1:
                            strategy_plan = parts[1].strip()
                    except:
                        pass
                
                # Construct the project object to return
                project_obj = {
                    "id": p['id'],
                    "project_name": p['project_name'],
                    "domain": p['domain'],
                    "language": p['language'],
                    "location": p['location'],
                    "focus": p['focus'],
                    "created_at": p['created_at'],
                    "business_summary": summary, # Cleaned summary
                    "strategy_plan": strategy_plan, # Extracted strategy
                    "ideal_customer_profile": profile.get('ideal_customer_profile'),
                    "brand_voice": profile.get('brand_voice'),
                    "primary_products": profile.get('primary_products'),
                    "competitors": profile.get('competitors'),
                    "unique_selling_points": profile.get('unique_selling_points'),
                    "page_count": counts.get(p['id'], 0),
                    "classified_count": classified_counts.get(p['id'], 0)
                }
                final_projects.append(project_obj)
            except Exception as e:
                print(f"Error processing project {p.get('id')}: {e}")
                continue
            
        return jsonify({"projects": final_projects})
    except Exception as e:
        print(f"Critical error in get_projects: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/get-pages', methods=['GET'])
def get_pages():
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        project_id = request.args.get('project_id')
        if not project_id:
            return jsonify({"error": "project_id is required"}), 400
        
        response = supabase.table('pages').select('*').eq('project_id', project_id).order('id').execute()
        
        import sys
        print(f"DEBUG: get_pages for {project_id} found {len(response.data) if response.data else 0} pages.", file=sys.stderr)
        
        # DEBUG: Check data structure
        if response.data:
            print(f"DEBUG: get_pages first row keys: {response.data[0].keys()}", file=sys.stderr)
            print(f"DEBUG: get_pages first row tech_audit_data: {response.data[0].get('tech_audit_data')}", file=sys.stderr)
            
        return jsonify({"pages": response.data})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/create-project', methods=['POST'])
def create_project():
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500
    
    data = request.get_json()
    print(f"DEBUG: create_project called with data: {data}")
    domain = data.get('domain')
    project_name = data.get('project_name', domain)
    language = data.get('language', 'English')
    location = data.get('location', 'US')
    focus = data.get('focus', 'Product')
    
    if not domain:
        return jsonify({"error": "Domain is required"}), 400
        
    try:
        # 1. Create Project
        print(f"Creating project for {domain}...")
        project_res = supabase.table('projects').insert({
            "domain": domain,
            "project_name": project_name,
            "language": language,
            "location": location,
            "focus": focus
        }).execute()
        
        if not project_res.data:
            raise Exception("Failed to create project")
            
        project_id = project_res.data[0]['id']
        print(f"Project created: {project_id}")
        
        # 2. Create Business Profile
        supabase.table('business_profiles').insert({
            "project_id": project_id,
            "domain": domain
        }).execute()
        
        return jsonify({
            "message": "Project created successfully",
            "project_id": project_id
        })
        
    except Exception as e:
        print(f"ERROR in create_project: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/api/classify-page', methods=['POST'])
def classify_page():
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500
        
    try:
        data = request.get_json()
        log_debug(f"DEBUG: classify_page received data: {data}")
        page_id = data.get('page_id')
        stage = data.get('stage') or data.get('funnel_stage')
        
        if not page_id or not stage:
            log_debug(f"DEBUG: Missing params. page_id={page_id}, stage={stage}")
            return jsonify({"error": "page_id and stage are required"}), 400
            
        # Try updating page_type instead of funnel_stage
        log_debug(f"DEBUG: Updating page_type to {stage} for {page_id}")
        
        update_data = {'page_type': stage}
        
        # ALWAYS set title from slug when moving to Product OR Category
        if stage == 'Product' or stage == 'Category':
            # Fetch current page data
            page_res = supabase.table('pages').select('*').eq('id', page_id).single().execute()
            if page_res.data:
                page = page_res.data
                tech_data = page.get('tech_audit_data')
                
                # Robust JSON parsing
                if isinstance(tech_data, str):
                    try:
                        import json
                        tech_data = json.loads(tech_data)
                    except:
                        tech_data = {}
                elif not tech_data:
                    tech_data = {}
                
                # ALWAYS extract title from URL slug, no matter what
                new_title = get_title_from_url(page['url'])
                print(f"DEBUG: Setting title to '{new_title}' for {page['url']}")
                
                # Update tech_data
                tech_data['title'] = new_title
                update_data['tech_audit_data'] = tech_data
                print(f"DEBUG: update_data payload: {update_data}")
        
        supabase.table('pages').update(update_data).eq('id', page_id).execute()
        
        return jsonify({"message": f"Page classified as {stage}"})

    except Exception as e:
        log_debug(f"DEBUG: classify_page error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/auto-classify', methods=['POST'])
def auto_classify():
    # Log to a separate file to ensure we see it
    with open('debug_classify.log', 'a') as f:
        f.write(f"DEBUG: ENTERING auto_classify\n")
    
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        data = request.get_json()
        project_id = data.get('project_id')
        if not project_id: return jsonify({"error": "project_id required"}), 400
        
        # Fetch all pages for project, ordered by ID to ensure "list order"
        res = supabase.table('pages').select('id, url, page_type, tech_audit_data').eq('project_id', project_id).order('id').execute()
        all_pages = res.data
        
        # Prioritize Unclassified pages
        unclassified_pages = [p for p in all_pages if p.get('page_type') in [None, 'Unclassified', 'Other', '']]
        
        # LIMIT: Only take the first 50 unclassified pages
        pages = unclassified_pages[:50]
        
        with open('debug_classify.log', 'a') as f:
            f.write(f"DEBUG: Total pages: {len(all_pages)}. Unclassified: {len(unclassified_pages)}. Processing batch of: {len(pages)}\n")
        
        updated_count = 0
        
        for p in pages:
            current_type = p.get('page_type')
            
            # Log every URL
            with open('debug_classify.log', 'a') as f:
                f.write(f"DEBUG: Processing {p['url']} | Type: {current_type}\n")

            # Allow overwriting if it's Unclassified, None, empty, OR 'Other'
            # We ONLY skip if it's already 'Product' or 'Category'
            if current_type in ['Product', 'Category']:
                with open('debug_classify.log', 'a') as f:
                    f.write(f"DEBUG: SKIPPING {p['url']} (Already {current_type})\n")
                continue
                
            url = p['url'].lower()
            new_type = None
            
            # 1. Check Technical Data (Most Accurate)
            tech_data = p.get('tech_audit_data') or {}
            og_type = tech_data.get('og_type', '').lower()
            
            if 'product' in og_type:
                new_type = 'Product'
            elif 'service' in og_type:
                new_type = 'Service'
            elif 'article' in og_type or 'blog' in og_type:
                new_type = 'Category'
            
            # 2. URL Heuristics (Fallback)
            if not new_type:
                # Strict Product
                if any(x in url for x in ['/product/', '/products/', '/item/', '/p/', '/shop/']):
                    new_type = 'Product'
                
                # Strict Service
                elif any(x in url for x in ['/service/', '/services/', '/solution/', '/solutions/', '/consulting/', '/offering/']):
                    new_type = 'Service'

                # Categories / Content
                elif any(x in url for x in ['/category/', '/categories/', '/c/', '/collection/', '/collections/', '/blog/', '/blogs/', '/article/', '/news/']):
                    new_type = 'Category'
                
                # Expanded Content (Generic E-commerce/Blog terms)
                # 'culture', 'trend', 'backstage', 'editorial', 'guide' are common content markers
                elif 'culture' in url or 'trend' in url or 'artistry' in url or 'how-to' in url or 'backstage' in url or 'collections' in url or 'editorial' in url or 'guide' in url:
                    new_type = 'Category'
                
                # Common Beauty/Fashion Categories (Generic)
                # lips, face, eyes, skincare, brushes are standard industry categories
                elif any(f"/{x}" in url for x in ['lips', 'face', 'eyes', 'brushes', 'skincare', 'bestsellers', 'new', 'sets', 'gifts']):
                    new_type = 'Category'
                
                # Keywords that imply a collection/list (Generic)
                elif 'shades' in url or 'colours' in url or 'looks' in url or 'inspiration' in url:
                    new_type = 'Category'
                
                # Generic "products" list pattern
                elif 'trending-products' in url or url.endswith('-products'):
                    new_type = 'Category'
            
            if new_type:
                with open('debug_classify.log', 'a') as f:
                    f.write(f"DEBUG: MATCH! {url} -> {new_type}\n")
            else:
                with open('debug_classify.log', 'a') as f:
                    f.write(f"DEBUG: NO MATCH for {url}\n")
            
            if new_type:
                supabase.table('pages').update({'page_type': new_type}).eq('id', p['id']).execute()
                updated_count += 1
                
        return jsonify({"message": f"Auto-classified {updated_count} pages", "count": updated_count})

    except Exception as e:
        print(f"Auto-classify error: {e}")
        return jsonify({"error": str(e)}), 500

def get_title_from_url(url):
    try:
        from urllib.parse import urlparse
        path = urlparse(url).path
        # Get last non-empty segment
        segments = [s for s in path.split('/') if s]
        if not segments: return "Home"
        slug = segments[-1]
        # Convert slug to title (e.g., "my-page-title" -> "My Page Title")
        return slug.replace('-', ' ').replace('_', ' ').title()
    except:
        return "Untitled Page"

def scrape_page_details(url):
    """Scrape detailed technical data for a single page."""
    import requests
    from bs4 import BeautifulSoup
    import time
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'
    }
    
    data = {
        'status_code': 0,
        'title': '',
        'meta_description': '',
        'h1': '',
        'canonical': '',
        'word_count': 0,
        'og_title': '',
        'og_description': '',
        'has_schema': False,
        'missing_alt_count': 0,
        'missing_h1': False,
        'onpage_score': 0,
        'load_time_ms': 0,
        'checks': [],
        'error': None
    }
    
    try:
        start_time = time.time()
        
        # Use curl to bypass TLS fingerprinting
        content = fetch_with_curl(url)
        data['load_time_ms'] = int((time.time() - start_time) * 1000)
        
        if content:
            data['status_code'] = 200 # Assume 200 if curl returns content
            soup = BeautifulSoup(content, 'html.parser')
            
            # Title
            # Robust extraction: Find first title not in SVG/Symbol
            page_title = None
            
            # 1. Try head > title first
            head_title = soup.select_one('head > title')
            if head_title and head_title.string:
                page_title = head_title
            
            # 2. Fallback: Search all titles and filter
            if not page_title:
                all_titles = soup.find_all('title')
                for t in all_titles:
                    # Check if parent or grandparent is SVG-related
                    parents = [p.name for p in t.parents]
                    if not any(x in ['svg', 'symbol', 'defs', 'g'] for x in parents):
                        page_title = t
                        break
            
            if page_title:
                data['title'] = page_title.get_text(strip=True)
            else:
                data['title'] = get_title_from_url(url)
                
            data['title_length'] = len(data['title'])
            
            # Meta Description
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                data['meta_description'] = meta_desc.get('content', '').strip()
                data['description_length'] = len(data['meta_description'])
                
            # H1
            h1 = soup.find('h1')
            if h1:
                data['h1'] = h1.get_text(strip=True)
            else:
                data['missing_h1'] = True
                data['checks'].append("Missing H1")
                
            # Canonical
            canonical = soup.find('link', attrs={'rel': 'canonical'})
            if canonical:
                data['canonical'] = canonical.get('href', '').strip()
            else:
                # Fallback regex for malformed HTML
                import re
                match = re.search(r'<link[^>]*rel=["\']canonical["\'][^>]*href=["\']([^"\']+)["\']', content)
                if match:
                    data['canonical'] = match.group(1).strip()
                
            # Word Count (rough estimate)
            text = soup.get_text(separator=' ')
            data['word_count'] = len(text.split())
            
        # Click Depth (Proxy: URL Depth)
            # Count slashes after the domain. 
            # e.g. https://domain.com/ = 0
            # https://domain.com/page = 1
            # https://domain.com/blog/post = 2
            from urllib.parse import urlparse
            parsed = urlparse(url)
            path = parsed.path.strip('/')
            data['click_depth'] = 0 if not path else len(path.split('/'))
            
            # OG Tags
            # Initialize with 'Missing' to allow fallback logic to work
            data['og_title'] = 'Missing'
            data['og_description'] = 'Missing'
            data['og_image'] = None

            og_title_tag = soup.find('meta', property='og:title') or soup.find('meta', attrs={'name': 'og:title'})
            if og_title_tag and og_title_tag.get('content'):
                data['og_title'] = og_title_tag['content'].strip()
            
            og_desc_tag = soup.find('meta', property='og:description') or soup.find('meta', attrs={'name': 'og:description'})
            if og_desc_tag and og_desc_tag.get('content'):
                data['og_description'] = og_desc_tag['content'].strip()
            
            og_image_tag = soup.find('meta', property='og:image') or soup.find('meta', attrs={'name': 'og:image'})
            if og_image_tag and og_image_tag.get('content'):
                data['og_image'] = og_image_tag['content'].strip()

            # FALLBACK: JSON-LD Schema (Common in Shopify/Wordpress if OG tags are missing/JS-rendered)
            if data['og_title'] == 'Missing' or data['og_description'] == 'Missing' or not data['og_image']:
                try:
                    import json
                    schemas = soup.find_all('script', type='application/ld+json')
                    for schema in schemas:
                        if not schema.string: continue
                        try:
                            json_data = json.loads(schema.string)
                            # Handle list of schemas
                            if isinstance(json_data, list):
                                items = json_data
                            else:
                                items = [json_data]
                                
                            for item in items:
                                # Prioritize Product, then Article, then WebPage
                                item_type = item.get('@type', '')
                                if isinstance(item_type, list): item_type = item_type[0] # Handle type as list
                                
                                if item_type in ['Product', 'Article', 'BlogPosting', 'WebPage']:
                                    if data['og_title'] == 'Missing' and item.get('name'):
                                        data['og_title'] = item['name']
                                        print(f"DEBUG: Recovered OG Title from Schema ({item_type})")
                                        
                                    if data['og_description'] == 'Missing' and item.get('description'):
                                        data['og_description'] = item['description']
                                        print(f"DEBUG: Recovered OG Desc from Schema ({item_type})")
                                        
                                    if not data['og_image'] and item.get('image'):
                                        img = item['image']
                                        if isinstance(img, list): img = img[0]
                                        elif isinstance(img, dict): img = img.get('url')
                                        data['og_image'] = img
                        except:
                            continue
                except Exception as e:
                    print(f"DEBUG: Schema parsing failed: {e}")

            # Schema
            schema = soup.find('script', type='application/ld+json')
            if schema: data['has_schema'] = True
            
            # Missing Alt Tags
            images = soup.find_all('img')
            for img in images:
                if not img.get('alt'):
                    data['missing_alt_count'] += 1
            
            # Calculate OnPage Score (Simple Heuristic)
            score = 100
            if data['missing_h1']: score -= 20
            if not data['title']: score -= 20
            if not data['meta_description']: score -= 20
            if data['missing_alt_count'] > 0: score -= min(10, data['missing_alt_count'] * 2)
            if data['word_count'] < 300: score -= 10
            if not data['og_title']: score -= 5
            if not data['og_description']: score -= 5
            
            data['onpage_score'] = max(0, score)
            
            # Technical Checks
            data['is_redirect'] = False # Cannot detect redirects easily with simple curl
            data['is_4xx_code'] = 400 <= data['status_code'] < 500
            data['is_5xx_code'] = 500 <= data['status_code'] < 600
            data['is_broken'] = data['status_code'] >= 400
            data['high_loading_time'] = data['load_time_ms'] > 30000 # Relaxed to 30s for Railway
            
            # Canonical Mismatch
            if data['canonical']:
                # Normalize URLs for comparison (remove trailing slash, etc)
                norm_url = url.rstrip('/')
                norm_canon = data['canonical'].rstrip('/')
                data['canonical_mismatch'] = norm_url != norm_canon
            else:
                data['canonical_mismatch'] = False # Or True if strict? Let's say False if missing.

    except Exception as e:
        data['error'] = str(e)
        data['is_broken'] = True
        print(f"Error scraping {url}: {e}")
        
    return data

def perform_tech_audit(project_id, limit=5):
    """Audit existing pages that are missing technical data."""
    print(f"Starting technical audit for project {project_id} (Limit: {limit})...")
    
    # 1. Get pages that need auditing (prioritize those without tech data)
    # Fetch all pages (or a large batch) and filter in python
    res = supabase.table('pages').select('id, url, tech_audit_data').eq('project_id', project_id).order('id').execute()
    all_pages = res.data
    
    # Filter for pages that have NO tech_audit_data, or "Pending Scan", or failed status (403/429)
    unaudited_pages = []
    for p in all_pages:
        tech = p.get('tech_audit_data') or {}
        status = tech.get('status_code')
        
        # Retry if:
        # 1. No data
        # 2. Title is missing or "Pending Scan"
        # 3. Status is Forbidden (403) or Rate Limited (429) or 0/None
        if not tech or \
           not tech.get('title') or \
           tech.get('title') == 'Pending Scan' or \
           status in [403, 429, 406, 0, None]:
            unaudited_pages.append(p)
            
    # Take the first 'limit' pages
    pages = unaudited_pages[:limit]
    print(f"DEBUG: Found {len(unaudited_pages)} unaudited pages. Processing first {len(pages)}.")
    
    audited_count = 0
    errors = []
    
    # Helper function for parallel execution
    def audit_single_page(p):
        try:
            url = p['url']
            print(f"DEBUG: Auditing {url}...")
            
            tech_data = scrape_page_details(url)
            # print(f"DEBUG: Scraped {url}. Status: {tech_data.get('status_code')}")
            
            # Merge with existing data
            existing_data = p.get('tech_audit_data') or {}
            existing_data.update(tech_data)
            
            # Update DB
            # print(f"DEBUG: Updating DB for {url}...")
            supabase.table('pages').update({
                'tech_audit_data': existing_data
            }).eq('id', p['id']).execute()
            
            print(f"DEBUG: Successfully audited {url}")
            return True, p
        except Exception as e:
            print(f"ERROR: Failed to audit {p.get('url')}: {e}")
            # Mark error in object for reporting
            if not p.get('tech_audit_data'): p['tech_audit_data'] = {}
            p['tech_audit_data']['error'] = str(e)
            return False, p

    # Execute sequentially (User requested efficiency/stability over speed)
    for p in pages:
        success, result_p = audit_single_page(p)
        if success:
            audited_count += 1
        else:
            errors.append(result_p)
        
    print(f"Audit complete. Updated {audited_count} pages.")
    return audited_count, errors

@app.route('/api/run-project-setup', methods=['POST'])
def run_project_setup():
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    data = request.json
    project_id = data.get('project_id')
    do_audit = data.get('do_audit', False)
    do_tech_audit = data.get('do_tech_audit', False)
    do_profile = data.get('do_profile', False)
    max_pages = data.get('max_pages', 200)
    
    if not project_id:
        return jsonify({"error": "Project ID required"}), 400
        
    try:
        # 1. Tech Audit (Standalone)
        if do_tech_audit:
            count, errors = perform_tech_audit(project_id)
            
            msg = f"Audit complete. Updated {count} pages."
            if count == 0 and len(errors) > 0:
                msg += " (Check console for details)"
                
            return jsonify({
                "message": msg,
                "count": count,
                "details": [f"Failed: {p.get('url')}" for p in errors if p.get('tech_audit_data', {}).get('error')]
            })

        if not project_id: return jsonify({"error": "project_id required"}), 400
        
        # Fetch Project Details
        proj_res = supabase.table('projects').select('*').eq('id', project_id).execute()
        if not proj_res.data: return jsonify({"error": "Project not found"}), 404
        project = proj_res.data[0]
        domain = project['domain']
        
        print(f"Starting Setup for {domain} (Audit: {do_audit}, Tech Audit: {do_tech_audit}, Profile: {do_profile}, Max Pages: {max_pages})...")
        
        profile_data = {}
        strategy_plan = ""
        profile_insert = {} # Initialize to empty dict
        
        # 0. Technical Audit (Deep Dive) - NEW
        if do_tech_audit:
             print(f"[SCRAPER] Starting technical audit for project {project_id}...")
             try:
                 count = perform_tech_audit(project_id, limit=max_pages)
                 print(f"[SCRAPER] ✅ Technical audit completed successfully. Audited {count} pages.")
                 return jsonify({"message": f"Technical audit completed for {count} pages.", "pages_audited": count})
             except Exception as audit_error:
                 error_msg = f"Technical audit failed: {str(audit_error)}"
                 print(f"[SCRAPER] ❌ ERROR: {error_msg}")
                 import traceback
                 traceback.print_exc()
                 return jsonify({"error": error_msg}), 500


        # 1. Research Business (The Brain)
        if do_profile:
            print("Starting Gemini research...")
            # try:
            #     tools = [{'google_search': {}}]
            #     model = genai.GenerativeModel('gemini-2.0-flash-exp', tools=tools)
            # except:
            #     print("Warning: Google Search tool failed. Using standard model.")
            #     model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            prompt = f"""
            You are an expert business analyst. Research the website {domain} and create a comprehensive Business Profile.
            
            Context:
            - Language: {project.get('language')}
            - Location: {project.get('location')}
            - Focus: {project.get('focus')}
            
            I need you to find:
            1. Business Summary: What do they do? (1 paragraph)
            2. Ideal Customer Profile (ICP): Who are they selling to? Be specific.
            3. Brand Voice: How do they sound?
            4. Primary Products: List their main products/services.
            5. Competitors: List 3-5 potential competitors.
            6. Unique Selling Points (USPs): What makes them different?
            
            Return JSON:
            {{
                "business_summary": "...",
                "ideal_customer_profile": "...",
                "brand_voice": "...",
                "primary_products": ["..."],
                "competitors": ["..."],
                "unique_selling_points": ["..."]
            }}
            """
            
            text = gemini_client.generate_content(
                prompt=prompt,
                model_name="gemini-2.5-flash",
                use_grounding=True
            )
            
            if not text:
                raise Exception("Gemini generation failed for Business Profile")
            
            # Parse JSON
            import json
            if text.startswith('```json'): text = text[7:]
            if text.startswith('```'): text = text[3:]
            if text.endswith('```'): text = text[:-3]
            
            profile_data = json.loads(text.strip())
            
            # 2. Generate Content Strategy Plan
            print("Generating Strategy Plan...")
            strategy_prompt = f"""
            Based on this business profile:
            {json.dumps(profile_data)}
            
            **CONTEXT**:
            - Target Audience Location: {project.get('location')}
            - Target Language: {project.get('language')}
            
            Create a high-level Content Strategy Plan following the "Bottom-Up" approach:
            1. Bottom Funnel (BoFu): What product/service pages need optimization?
            2. Middle Funnel (MoFu): What comparison/best-of topics link to BoFu?
            3. Top Funnel (ToFu): What informational topics link to MoFu?
            
            Return a short markdown summary of the strategy.
            """
            strategy_plan = gemini_client.generate_content(
                prompt=strategy_prompt,
                model_name="gemini-2.5-flash",
                use_grounding=True
            )
            if not strategy_plan: strategy_plan = ""
            
            # Save Business Profile
            # WORKAROUND: Append Strategy Plan to Business Summary for persistence
            combined_summary = profile_data.get("business_summary", "")
            if strategy_plan:
                combined_summary += "\n\n===STRATEGY_PLAN===\n\n" + strategy_plan

            profile_insert = {
                "project_id": project_id,
                "business_summary": combined_summary,
                "ideal_customer_profile": profile_data.get("ideal_customer_profile"),
                "brand_voice": profile_data.get("brand_voice"),
                "primary_products": profile_data.get("primary_products"),
                "competitors": profile_data.get("competitors"),
                "unique_selling_points": profile_data.get("unique_selling_points")
            }
            
            # Check if exists, update or insert
            existing = supabase.table('business_profiles').select('id').eq('project_id', project_id).execute()
            if existing.data:
                supabase.table('business_profiles').update(profile_insert).eq('id', existing.data[0]['id']).execute()
            else:
                supabase.table('business_profiles').insert(profile_insert).execute()

        # 3. Crawl Sitemap (The Map)
        pages_to_insert = []
        
        if do_audit:
            print(f"Starting sitemap crawl (Audit enabled, Max Pages: {max_pages})...")
            pages_to_insert = crawl_sitemap(domain, project_id, max_pages=max_pages)
            
            if pages_to_insert:
                print(f"Found {len(pages_to_insert)} pages. syncing with DB...")
                
                # 1. Get existing URLs to avoid duplicates
                existing_res = supabase.table('pages').select('url, id, tech_audit_data').eq('project_id', project_id).execute()
                existing_map = {row['url']: row for row in existing_res.data}
                
                new_pages = []
                
                for p in pages_to_insert:
                    url = p['url']
                    if url in existing_map:
                        # Update existing page if title is missing or we have a better one
                        existing_row = existing_map[url]
                        existing_data = existing_row.get('tech_audit_data') or {}
                        new_data = p.get('tech_audit_data') or {}
                        
                        # If existing has no title, or we want to refresh it
                        if not existing_data.get('title') or new_data.get('title') != 'Untitled Product':
                            # Merge data
                            updated_data = existing_data.copy()
                            updated_data.update(new_data)
                            
                            # Only update if changed
                            if updated_data != existing_data:
                                print(f"Updating title for {url}")
                                supabase.table('pages').update({'tech_audit_data': updated_data}).eq('id', existing_row['id']).execute()
                    else:
                        new_pages.append(p)
                
                # 2. Insert only new pages
                if new_pages:
                    print(f"Inserting {len(new_pages)} new pages...")
                    batch_size = 100
                    for i in range(0, len(new_pages), batch_size):
                        batch = new_pages[i:i+batch_size]
                        supabase.table('pages').insert(batch).execute()
                    
                #3. Update project page_count field (DISABLED - column doesn't exist in schema)
                # total_pages = supabase.table('pages').select('*', count='exact').eq('project_id', project_id).execute()
                # supabase.table('projects').update({
                #     'page_count': total_pages.count
                # }).eq('id', project_id).execute()
                # print(f"Updated project page_count to {total_pages.count}")
                print(f"Inserted {len(new_pages)} new pages successfully.")
        else:
            print("Audit disabled. Skipping crawl.")
                
        return jsonify({
            "message": "Project setup complete",
            "profile": profile_insert,
            "strategy_plan": strategy_plan,
            "pages_found": len(pages_to_insert),
            "audit_run": do_audit
        })

    except Exception as e:
        print(f"Error in run_project_setup: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/generate-funnel', methods=['POST'])
def generate_funnel():
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500
        
    try:
        data = request.get_json()
        page_id = data.get('page_id')
        project_id = data.get('project_id')
        current_stage = data.get('current_stage', 'BoFu') # Default to BoFu if not sent
        
        if not page_id or not project_id:
            return jsonify({"error": "page_id and project_id are required"}), 400
            
        # 1. Fetch Context
        profile_res = supabase.table('business_profiles').select('*').eq('project_id', project_id).execute()
        profile = profile_res.data[0] if profile_res.data else {}
        
        page_res = supabase.table('pages').select('*').eq('id', page_id).execute()
        page = page_res.data[0] if page_res.data else {}
        
        target_stage = "MoFu" if current_stage == 'BoFu' else "ToFu"
        
        print(f"Generating {target_stage} strategy for {page.get('url')}...")
        
        # 2. Prompt Gemini
        prompt = f"""
        You are a strategic SEO expert for this business:
        Summary: {profile.get('business_summary')}
        ICP: {profile.get('ideal_customer_profile')}
        
        We are building a Content Funnel.
        Current Page ({current_stage}): {page.get('title')} ({page.get('url')})
        
        Task: Generate 5 high-impact "{target_stage}" content ideas that will drive traffic to this Current Page.
        
        Definitions:
        - If Target is MoFu (Middle of Funnel): Generate "Comparison", "Best X for Y", or "Alternative to Z" articles. These help users evaluate options.
        - If Target is ToFu (Top of Funnel): Generate "How-to", "What is", or "Guide" articles. These help users understand the problem.
        
        Output JSON format:
        [
            {{
                "topic_title": "Title of the article",
                "primary_keyword": "Main SEO keyword",
                "rationale": "Why this drives traffic to the parent page"
            }}
        ]
        """
        
        # model = genai.GenerativeModel('gemini-2.0-flash-exp')
        # response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
        
        text = gemini_client.generate_content(
            prompt=prompt,
            model_name="gemini-2.5-flash",
            use_grounding=True
        )
        
        if not text:
            raise Exception("Gemini generation failed for Content Strategy")
            
        # Clean markdown
        if text.startswith('```json'): text = text[7:]
        if text.startswith('```'): text = text[3:]
        if text.endswith('```'): text = text[:-3]
        
        # 3. Parse and Save
        ideas = json.loads(text.strip())
        
        # 4. Enrich with DataForSEO (Optional)
        try:
            keywords = [idea.get('primary_keyword') for idea in ideas if idea.get('primary_keyword')]
            keyword_data = fetch_keyword_data(keywords)
        except Exception as e:
            print(f"DataForSEO Error: {e}")
            keyword_data = {}

        briefs_to_insert = []
        for idea in ideas:
            kw = idea.get('primary_keyword')
            data = keyword_data.get(kw, {})
            
            briefs_to_insert.append({
                "project_id": project_id,
                "topic_title": idea.get('topic_title'),
                "primary_keyword": kw,
                "rationale": idea.get('rationale'),
                "parent_page_id": page_id, 
                "status": "Proposed",
                "funnel_stage": target_stage,
                "meta_data": data # Store volume/kd here
            })
            
        if briefs_to_insert:
            supabase.table('content_briefs').insert(briefs_to_insert).execute()
            
            # SYNC TO PAGES TABLE (Fix for Dashboard Visibility)
            pages_to_insert = []
            for brief in briefs_to_insert:
                pages_to_insert.append({
                    "project_id": project_id,
                    "url": f"pending-slug-{uuid.uuid4()}", # Placeholder URL
                    "page_type": "Topic",
                    "funnel_stage": target_stage,
                    "source_page_id": page_id,
                    "tech_audit_data": {"title": brief['topic_title']},
                    "content_description": brief['rationale'],
                    "keywords": brief['primary_keyword']
                })
            
            if pages_to_insert:
                print(f"Syncing {len(pages_to_insert)} topics to pages table...")
                supabase.table('pages').insert(pages_to_insert).execute()
            
        return jsonify({
            "message": f"Generated {len(briefs_to_insert)} {target_stage} ideas",
            "ideas": briefs_to_insert
        })

    except Exception as e:
        print(f"Error in generate_funnel: {e}")
        return jsonify({"error": str(e)}), 500

def fetch_keyword_data(keywords):
    if not keywords: 
        print("No keywords provided to fetch_keyword_data")
        return {}
    
    login = os.environ.get('DATAFORSEO_LOGIN')
    password = os.environ.get('DATAFORSEO_PASSWORD')
    
    print(f"DataForSEO Login: {login}, Password: {'*' * len(password) if password else 'None'}")
    
    if not login or not password:
        print("DataForSEO credentials missing")
        return {}
        
    url = "https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live"
    
    # We can use 'keyword_ideas' or 'keywords_for_site' or just 'search_volume'
    # 'search_volume' is best for specific lists.
    url = "https://api.dataforseo.com/v3/dataforseo_labs/google/historical_search_volume/live"
    
    payload = [{
        "keywords": keywords,
        "location_code": 2840, # US
        "language_code": "en"
    }]
    
    try:
        print(f"Fetching keyword data for {len(keywords)} keywords: {keywords[:3]}...")
        response = requests.post(url, auth=(login, password), json=payload)
        res_data = response.json()
        
        print(f"DataForSEO Response Status: {response.status_code}")
        print(f"DataForSEO Response: {res_data}")
        
        result = {}
        if res_data.get('tasks') and len(res_data['tasks']) > 0:
            task_result = res_data['tasks'][0].get('result')
            if task_result:
                for item in task_result:
                    kw = item.get('keyword')
                    vol = item.get('search_volume', 0)
                    result[kw] = {"volume": vol}
                    print(f"Keyword '{kw}': Volume = {vol}")
            else:
                print("No result in task")
        else:
            print("No tasks in response")
                
        return result
        
    except Exception as e:
        print(f"DataForSEO Request Failed: {e}")
        import traceback
        traceback.print_exc()
        return {}

def validate_and_enrich_keywords(ai_keywords_str, topic_title, min_volume=100):
    """
    Validates AI-generated keywords against DataForSEO search volume data.
    Replaces low-volume keywords with high-value alternatives.
    
    Args:
        ai_keywords_str: Comma-separated keyword string from AI
        topic_title: Topic title to use for finding alternatives if needed
        min_volume: Minimum monthly search volume threshold (default: 100)
    
    Returns:
        str: Comma-separated validated keywords with volume annotations
    """
    if not ai_keywords_str:
        return ""
    
    # Parse AI keywords
    ai_keywords = [k.strip() for k in ai_keywords_str.split(',') if k.strip()]
    if not ai_keywords:
        return ""
    
    print(f"Validating {len(ai_keywords)} AI keywords: {ai_keywords[:3]}...")
    
    # Fetch search volume data
    keyword_data = fetch_keyword_data(ai_keywords)
    
    # Filter and format keywords with volume
    validated_keywords = []
    for kw in ai_keywords:
        data = keyword_data.get(kw, {})
        volume = data.get('volume', 0)
        
        if volume >= min_volume:
            validated_keywords.append(f"{kw} (Vol: {volume})")
            print(f"✓ Kept '{kw}' - Volume: {volume}")
        else:
            print(f"✗ Rejected '{kw}' - Volume: {volume} (below threshold)")
    
    # If we have fewer than 3 good keywords, try to find alternatives
    if len(validated_keywords) < 3:
        print(f"Only {len(validated_keywords)} validated keywords. Searching for alternatives...")
        
        try:
            login = os.environ.get('DATAFORSEO_LOGIN')
            password = os.environ.get('DATAFORSEO_PASSWORD')
            
            if login and password:
                url = "https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live"
                payload = [{
                    "keywords": [topic_title],
                    "location_code": 2840,
                    "language_code": "en",
                    "include_seed_keyword": False,
                    "filters": [
                        ["keyword_data.keyword_info.search_volume", ">=", min_volume]
                    ],
                    "order_by": ["keyword_data.keyword_info.search_volume,desc"],
                    "limit": 10
                }]
                
                response = requests.post(url, auth=(login, password), json=payload)
                res_data = response.json()
                
                if res_data.get('tasks') and res_data['tasks'][0].get('result'):
                    for item in res_data['tasks'][0]['result'][0].get('items', []):
                        kw = item['keyword']
                        volume = item['keyword_data']['keyword_info']['search_volume']
                        
                        # Avoid duplicates
                        if not any(kw.lower() in vk.lower() for vk in validated_keywords):
                            validated_keywords.append(f"{kw} (Vol: {volume})")
                            print(f"+ Added alternative '{kw}' - Volume: {volume}")
                            
                            if len(validated_keywords) >= 5:
                                break
        except Exception as e:
            print(f"Error fetching keyword alternatives: {e}")
    
    # Return top 5 validated keywords
    result = ', '.join(validated_keywords[:5])
    print(f"Final validated keywords: {result}")
    return result



def analyze_serp_for_keyword(keyword, location_code=2840):
    """
    Fetches top 10 SERP results for a keyword using DataForSEO.
    Returns competitor data: titles, URLs, ranking positions.
    """
    login = os.environ.get('DATAFORSEO_LOGIN')
    password = os.environ.get('DATAFORSEO_PASSWORD')
    
    if not login or not password:
        print("DataForSEO credentials missing for SERP analysis")
        return []
    
    try:
        url = "https://api.dataforseo.com/v3/serp/google/organic/live/advanced"
        payload = [{
            "keyword": keyword,
            "location_code": location_code,
            "language_code": "en",
            "device": "desktop",
            "depth": 10
        }]
        
        print(f"Analyzing SERP for '{keyword}'...")
        response = requests.post(url, auth=(login, password), json=payload)
        data = response.json()
        
        competitors = []
        if data.get('tasks') and data['tasks'][0].get('result') and data['tasks'][0]['result'][0].get('items'):
            for item in data['tasks'][0]['result'][0]['items']:
                if item.get('type') == 'organic':
                    competitors.append({
                        'url': item.get('url'),
                        'title': item.get('title'),
                        'position': item.get('rank_absolute'),
                        'domain': item.get('domain')
                    })
                    print(f"  #{item.get('rank_absolute')}: {item.get('domain')} - {item.get('title')}")
        
        print(f"Found {len(competitors)} competitors for '{keyword}'")
        return competitors
        
    except Exception as e:
        print(f"SERP analysis error for '{keyword}': {e}")
        import traceback
        traceback.print_exc()
        return []

        return []


def perform_gemini_research(topic, location="US", language="English"):
    """
    Uses Gemini 2.0 Flash with Google Search Grounding to perform free research.
    Returns structured data: {
        "competitors": [{"url": "...", "title": "...", "domain": "..."}],
        "keywords": [{"keyword": "...", "intent": "...", "volume": "N/A"}],
        "research_brief": "Markdown content...",
        "citations": ["url1", "url2"]
    }
    """
    log_debug(f"Starting Gemini Free Research for: {topic} (Loc: {location}, Lang: {language})")
    
    try:

        # Use gemini_client for pure REST API calls (No SDK)
        
        prompt = f"""
        Research the SEO topic: "{topic}"
        
        **CONTEXT**:
        - Target Audience Location: {location}
        - Target Language: {language}
        
        Perform a deep analysis using Google Search to find:
        1. Top 3 Competitor URLs ranking for this topic in **{location}**.
        2. **At least 30 SEO Keywords** relevant to this topic (include Search Intent).
           - Focus on keywords trending in **{location}**.
           - Mix of short-tail and long-tail.
           - Include "People Also Ask" style questions relevant to this region.
           
        **PRIORITIZATION RULES**:
        1. **Primary Focus**: Prioritize keywords specifically trending in **{location}**.
        2. **Global Keywords**: You MAY include high-volume US/Global keywords if they are highly relevant, but they must be secondary to local terms.
        3. **Relevance**: Ensure all keywords are actionable for a user in {location}.
        
        Output strictly in JSON format:
        {{
            "competitors": [
                {{"url": "https://...", "title": "Page Title", "domain": "domain.com"}}
            ],
            "keywords": [
                {{"keyword": "keyword phrase", "intent": "Informational/Commercial/Transactional"}}
            ]
        }}
        """
        
        text = gemini_client.generate_content(
            prompt=prompt,
            model_name="gemini-2.5-flash",
            use_grounding=True
        )
        
        if not text:
            raise Exception("Empty response from Gemini REST API")
        
        # Clean markdown code blocks if present
        if text.startswith('```json'): text = text[7:]
        if text.startswith('```'): text = text[3:]
        if text.endswith('```'): text = text[:-3]
            
        return json.loads(text.strip())
        
    except Exception as e:
        log_debug(f"Gemini Research Error: {e}")
        print(f"Gemini Research Error: {e}")
        return None


def research_with_perplexity(query, location="US", language="English"):
    """
    Uses Perplexity Sonar to get verifiable research with citations.
    Returns structured research data with source URLs.
    """
    log_debug(f"research_with_perplexity called (Loc: {location}, Lang: {language})")
    api_key = os.environ.get('PERPLEXITY_API_KEY')
    
    if not api_key:
        log_debug("Perplexity API key missing - skipping research")
        print("Perplexity API key missing - skipping research")
        return {"research": "Perplexity API not configured", "citations": []}
    
    log_debug(f"Perplexity API key found: {api_key[:10]}...")
    
    try:
        url = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "sonar-pro",  # Using deep research model
            "messages": [{
                "role": "user",
                "content": f"""**Role**: You are a Senior Content Strategist and Market Researcher conducting deep-dive competitive analysis.

**Objective**: Create a comprehensive Research Brief for a Middle-of-Funnel (MoFu) content asset. This must be the MOST authoritative resource on this topic, outranking all competitors with superior data, utility, and insight.

**CONTEXT**:
- Target Audience Location: {location}
- Target Language: {language}

**LOCALIZATION RULES (CRITICAL)**:
1. **Currency**: You MUST use the local currency for **{location}** (e.g., ₹ INR for India). Convert any research prices (like $) to the local currency using approximate current rates.
2. **Units**: Use the measurement system standard for **{location}**.
3. **Spelling**: Use the correct spelling dialect (e.g., "Colour" for UK/India).

{query}

**CRITICAL RULES**:
- GENERATE A COMPLETE BRIEF based on the provided data and your general knowledge
- Use the provided competitor URLs and scraped text as your primary source
- If specific data is missing, use INDUSTRY BENCHMARKS or GENERAL CATEGORY KNOWLEDGE relevant to **{location}**
- Do not refuse to generate sections - provide the best available estimates
- Format as markdown with ## headers

---

## 1. Strategic Overview

**Proposed Title**: [SEO-optimized H1 using "Best X for Y 2025" or "Product A vs B vs C" format]

**Search Intent**: [Analyze based on the provided keyword list: Informational/Commercial/Transactional]

**Format Strategy**: [Why this format fits the MoFu stage]

---

## 2. Key Insights & Benchmarks (The Evidence)

**Market Data & Specifications** (Extract from content or use category knowledge):
- [Key Feature/Spec 1]: [Value/Description]
- [Key Feature/Spec 2]: [Value/Description]
- [Price Range]: [Estimated category range]
- [User Ratings]: [Typical sentiment/rating]
- [Technical Specs]: [Ingredients, dimensions, etc.]

**Expert/Industry Concepts**:
- [Key Concept 1]: [Explanation]
- [Key Concept 2]: [Explanation]

---

## 3. Competitor Landscape & Content Gaps

**Competitor Analysis** (Based on provided URLs):
- **Competitor 1**: [Name/URL]
  - Strengths: [What they cover well]
  - Weaknesses: [What they miss]
- **Competitor 2**: [Name/URL]
  - Strengths: [What they cover well]
  - Weaknesses: [What they miss]

**The "Blue Ocean" Gap**: [The ONE angle or utility missing from the above competitors. E.g., "No one compares X vs Y directly" or "Missing detailed ingredient breakdown"]

---

## 4. Comprehensive Content Outline

**Type**: [Comparison Guide / Buying Guide / Ultimate Guide]

**Title**: [Final SEO-optimized H1]

**Detailed Structure**:

### H2: Introduction
- Hook: [Problem/Stat]
- Scope: [What's covered]

### H2: [Main Section 1 - Category Overview]
- H3: [Subtopic from keyword list]
  - **Key Point**: [Detail]
- H3: [Subtopic from keyword list]
  - **Key Point**: [Detail]

### H2: [Comparison Section]
- H3: Comparison Chart
  - **Columns**: [Attribute 1], [Attribute 2], [Attribute 3]
  - **Data Source**: [Competitor content or benchmarks]
- H3: [Product A] vs [Competitors]
  - **Differentiator**: [Specific advantage]

### H2: [Buying Guide / Selection Criteria]
- H3: Who is this for?
  - **User Type 1**: [Recommendation]
  - **User Type 2**: [Recommendation]

### H2: FAQ
- [Question from keyword list]: [Answer]
- [Question from keyword list]: [Answer]

### H2: Conclusion
- Final Recommendation
- CTA

---

## 5. Unique Ranking Hypothesis

[Explain why this content will outrank competitors based on the gaps identified above. Focus on: Better data, clearer structure, or more comprehensive scope.]

**GENERATE THE COMPLETE BRIEF NOW.**
"""
            }],
            "return_citations": True,
            "search_recency_filter": "month"
        }
        
        log_debug(f"Calling Perplexity API with query: {query[:50]}...")
        print(f"Researching with Perplexity: {query[:100]}...")
        # Increased timeout to 120s for deep research
        response = requests.post(url, headers=headers, json=payload, timeout=120)
        log_debug(f"Perplexity response status: {response.status_code}")
        
        data = response.json()
        
        if 'choices' in data and len(data['choices']) > 0:
            content = data['choices'][0]['message']['content']
            citations = data.get('citations', [])
            
            log_debug(f"✓ Perplexity success! {len(citations)} citations")
            print(f"✓ Research completed. Found {len(citations)} citations")
            for i, cite in enumerate(citations[:3]):
                print(f"  Citation {i+1}: {cite}")
            
            return {
                "research": content,
                "citations": citations
            }
        else:
            log_debug(f"Unexpected Perplexity response structure: {str(data)[:200]}")
            print(f"Unexpected Perplexity response: {data}")
            return {"research": "Research failed", "citations": []}
            
    except Exception as e:
        log_debug(f"Perplexity error: {type(e).__name__} - {str(e)}")
        print(f"Perplexity research error: {e}")
        import traceback
        traceback.print_exc()
        return {"research": f"Error: {str(e)}", "citations": []}


def get_keyword_ideas(seed_keyword, location_code=2840, min_volume=100, limit=20):
    """
    Gets keyword ideas from DataForSEO based on a seed keyword.
    Returns list of keywords scored by (Volume × CPC) / Competition.
    Prioritizes high-intent, low-competition opportunities.
    """
    login = os.environ.get('DATAFORSEO_LOGIN')
    password = os.environ.get('DATAFORSEO_PASSWORD')
    
    if not login or not password:
        print("DataForSEO credentials missing for keyword research")
        return []
    
    try:
        url = "https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live"
        payload = [{
            "keywords": [seed_keyword],
            "location_code": location_code,
            "language_code": "en",
            "include_seed_keyword": True,
            "limit": 100
        }]
        
        print(f"Finding keyword ideas for '{seed_keyword}'...")
        log_debug(f"DataForSEO request: seed='{seed_keyword}', location={location_code}, min_vol={min_volume}")
        response = requests.post(url, auth=(login, password), json=payload, timeout=30)
        log_debug(f"DataForSEO status: {response.status_code}")
        data = response.json()
        
        keywords = []
        if data.get('tasks') and data['tasks'][0].get('result') and data['tasks'][0]['result'][0].get('items'):
            items = data['tasks'][0]['result'][0]['items']
            log_debug(f"DataForSEO returned {len(items)} items")
            
            for item in items:
                kw = item.get('keyword')
                
                # Robust extraction for info
                info = {}
                if 'keyword_info' in item:
                    info = item['keyword_info']
                elif 'keyword_data' in item and 'keyword_info' in item['keyword_data']:
                    info = item['keyword_data']['keyword_info']
                
                if not kw or not info:
                    log_debug(f"Skipping {kw}: Missing info")
                    continue
                    
                volume = info.get('search_volume', 0)
                if volume is None: volume = 0
                
                # Filter by min_volume in Python (can't use filters param)
                if volume < min_volume:
                    log_debug(f"Skipping {kw}: Low volume {volume} < {min_volume}")
                    continue
                
                cpc = info.get('cpc', 0.01) or 0.01
                competition = info.get('competition', 0.5) or 0.5
                
                # Smart scoring: (Volume × CPC) / Competition
                score = (volume * cpc) / max(competition, 0.1)
                
                keywords.append({
                    'keyword': kw,
                    'volume': volume,
                    'cpc': cpc,
                    'competition': competition,
                    'score': round(score, 2)
                })
        else:
            log_debug(f"DataForSEO returned NO items. Response structure: {str(data)[:300]}")
        
        # Sort by score (best opportunities first)
        keywords.sort(key=lambda x: x['score'], reverse=True)
        
        # Return top N
        top_keywords = keywords[:limit]
        
        log_debug(f"Returning {len(top_keywords)} keywords (from {len(keywords)} total)")
        print(f"Found {len(keywords)} keywords, returning top {len(top_keywords)} by opportunity score:")
        for kw in top_keywords[:5]:
            print(f"  {kw['keyword']}: Vol={kw['volume']}, CPC=${kw['cpc']:.2f}, Comp={kw['competition']:.2f}, Score={kw['score']}")
        
        return top_keywords
        
    except Exception as e:
        print(f"Keyword research error: {e}")
        import traceback
        traceback.print_exc()
        return []


def get_serp_competitors(keyword, location_code=2840, limit=5):
    """
    Gets top ranking URLs for a keyword using DataForSEO SERP API.
    Returns list of competitor URLs with titles and domains.
    """
    login = os.environ.get('DATAFORSEO_LOGIN')
    password = os.environ.get('DATAFORSEO_PASSWORD')
    
    if not login or not password:
        log_debug("DataForSEO credentials missing for SERP API")
        return []
    
    try:
        url = "https://api.dataforseo.com/v3/serp/google/organic/live/advanced"
        payload = [{
            "keyword": keyword,
            "location_code": location_code,
            "language_code": "en",
            "depth": 20,
            "device": "desktop"
        }]
        
        log_debug(f"SERP API: Finding competitors for '{keyword}'")
        response = requests.post(url, auth=(login, password), json=payload, timeout=30)
        log_debug(f"SERP API status: {response.status_code}")
        
        data = response.json()
        
        competitors = []
        if data.get('tasks') and data['tasks'][0].get('result'):
            results = data['tasks'][0]['result']
            if results and len(results) > 0 and results[0].get('items'):
                items = results[0]['items']
                log_debug(f"SERP API returned {len(items)} total items")
                
                for item in items:
                    if len(competitors) >= limit:
                        break
                        
                    # Only look at organic results
                    if item.get('type') != 'organic':
                        continue
                        
                    url_data = item.get('url')
                    title = item.get('title', '')
                    domain = item.get('domain', '')
                    
                    # Skip blocklisted domains
                    if any(b in domain for b in ['amazon', 'ebay', 'walmart', 'youtube', 'pinterest', 'instagram', 'facebook', 'reddit', 'quora']):
                        continue
                    
                    if url_data and domain:
                        competitors.append({
                            'url': url_data,
                            'title': title,
                            'domain': domain,
                            'position': item.get('rank_group', 0)
                        })
        
        log_debug(f"SERP API returned {len(competitors)} competitors")
        return competitors
        
    except Exception as e:
        log_debug(f"SERP API error: {type(e).__name__} - {str(e)}")
        print(f"SERP API error: {e}")
        return []


def get_ranked_keywords_for_url(target_url, location_code=2840, limit=100):
    """
    Gets keywords that a specific URL ranks for using DataForSEO Ranked Keywords API.
    This generates the keyword list format: "keyword | intent | secondary intent"
    """
    login = os.environ.get('DATAFORSEO_LOGIN')
    password = os.environ.get('DATAFORSEO_PASSWORD')
    
    if not login or not password:
        log_debug("DataForSEO credentials missing for Ranked Keywords API")
        return []
    
    try:
        url = "https://api.dataforseo.com/v3/dataforseo_labs/google/ranked_keywords/live"
        payload = [{
            "target": target_url,
            "location_code": location_code,
            "language_code": "en",
            "limit": limit
            # Removed order_by - causes 40501 error
        }]
        
        log_debug(f"Ranked Keywords API: Getting keywords for '{target_url[:50]}...'")
        response = requests.post(url, auth=(login, password), json=payload, timeout=30)
        log_debug(f"Ranked Keywords API status: {response.status_code}")
        
        data = response.json()
        
        keywords = []
        if data.get('tasks') and data['tasks'][0].get('result'):
            results = data['tasks'][0]['result']
            if results and len(results) > 0 and results[0].get('items'):
                for item in results[0]['items']:
                    # Robust extraction for keyword
                    keyword = item.get('keyword_data', {}).get('keyword')
                    if not keyword:
                        keyword = item.get('keyword')
                    
                    # Robust extraction for position
                    position = 999
                    if 'metrics' in item and 'organic' in item['metrics']:
                        position = item['metrics']['organic'].get('pos_1', 999)
                    elif 'ranked_serp_element' in item:
                         position = item['ranked_serp_element'].get('serp_item', {}).get('rank_group', 999)
                    
                    # Debug filtering
                    domain = item.get('ranked_serp_element', {}).get('serp_item', {}).get('domain', 'unknown')
                    if position > 30:
                        log_debug(f"Skipping {domain}: Rank {position} > 30")
                        continue
                        
                    # Check blocklist
                    if any(b in domain for b in ['amazon', 'ebay', 'walmart', 'youtube', 'pinterest', 'instagram', 'facebook', 'reddit', 'quora']):
                        log_debug(f"Skipping {domain}: Blocklisted")
                        continue
                        
                    log_debug(f"Keeping {domain} (Rank {position})")
                    
                    # Classify intent
                    kw_lower = keyword.lower()
                    intents = []
                    
                    # Try to get intent from API
                    api_intent = item.get('keyword_data', {}).get('keyword_info', {}).get('search_intent')
                    if api_intent:
                        intent_str = api_intent
                    else:
                        # Fallback to rule-based
                        intents = []
                        if any(w in kw_lower for w in ['buy', 'price', 'shop', 'purchase', 'order', 'discount', 'sale', 'deal', 'cheap', 'cost']):
                            intents.append('transactional')
                        if any(w in kw_lower for w in ['best', 'top', 'review', 'vs', 'compare', 'alternative', 'guide', 'list', 'ranking']):
                            intents.append('commercial')
                        if any(w in kw_lower for w in ['what', 'how', 'benefits', 'made from', 'function', 'define', 'meaning', 'examples']):
                            intents.append('informational')
                        
                        if not intents:
                            intents.append('informational')
                        
                        intent_str = ', '.join(intents)
                    
                    if keyword:
                        keywords.append({
                            'keyword': keyword,
                            'intent': intent_str,
                            'position': position
                        })
        
        log_debug(f"Ranked Keywords API returned {len(keywords)} keywords")
        return keywords
        
    except Exception as e:
        log_debug(f"Ranked Keywords API error: {type(e).__name__} - {str(e)}")
        print(f"Ranked Keywords API error: {e}")
        return []






import uuid # Added for filename generation

@app.route('/api/upload', methods=['POST'])
def upload_image():
    if 'file' not in request.files:
        return jsonify({"error": "No file part"}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
        
    if file:
        try:
            # Read file data
            file_data = file.read()
            filename = f"{uuid.uuid4()}_{file.filename}"
            
            # Upload to Supabase
            public_url = upload_to_supabase(file_data, filename)
            
            return jsonify({"url": public_url})
        except Exception as e:
            print(f"Upload error: {e}")
            return jsonify({"error": str(e)}), 500



@app.route('/api/generate-image', methods=['POST'])
def generate_image_endpoint():
    data = request.json
    prompt = data.get('prompt')
    input_image_url = data.get('input_image_url')
    
    if not prompt:
        return jsonify({'error': 'Prompt is required'}), 400

    try:
        # 1. Enhance Prompt using Gemini (Text)
        # We can use the new client for this too, or stick to old one. 
        # Let's use the new client for consistency if possible, but mixing is fine for now to minimize risk.
        # Actually, let's just use the new client for image gen as tested.
        
        enhanced_prompt = prompt 
        # (Optional: Add enhancement logic back if needed, but for now direct is fine or we can re-add it)
        # The previous code used `model = genai.GenerativeModel("gemini-2.0-flash-exp")` from old SDK.
        # Let's keep the enhancement logic using the old SDK if it works, or switch to new.
        # To avoid conflict, let's just use the prompt directly for now to ensure image gen works, 
        # or use the new client for text generation too.
        
        UPLOAD_FOLDER = os.path.join('public', 'uploads')
        os.makedirs(UPLOAD_FOLDER, exist_ok=True)

        output_filename = f"gen_{uuid.uuid4()}.png"
        output_path = os.path.join(UPLOAD_FOLDER, output_filename)
        
        print(f"Generating image for prompt: {prompt}")
        
        result_path = gemini_client.generate_image(
            prompt=prompt,
            output_path=output_path,
            model_name="gemini-2.5-flash-image"
        )
        
        if not result_path:
            raise Exception("Gemini Image API failed")
            
        # Continue with existing logic (which expects output_filename)
        # We need to ensure the file exists at output_path, which generate_image does.
        
        UPLOAD_FOLDER = os.path.join('public', 'uploads')
        os.makedirs(UPLOAD_FOLDER, exist_ok=True)

        output_filename = f"gen_{uuid.uuid4()}.png"
        output_path = os.path.join(UPLOAD_FOLDER, output_filename)
        
        image_saved = False
        if response.candidates and response.candidates[0].content.parts:
            for part in response.candidates[0].content.parts:
                if part.inline_data:
                    data = part.inline_data.data
                    if isinstance(data, str):
                        image_data = base64.b64decode(data)
                    else:
                        image_data = data
                        
                    with open(output_path, "wb") as f:
                        f.write(image_data)
                    image_saved = True
                    break
        
        if not image_saved:
            return jsonify({'error': 'No image generated'}), 500

        # Return URL
        output_url = f"/uploads/{output_filename}"
        
        return jsonify({
            'output_image_url': output_url,
            'status': 'Done',
            'enhanced_prompt': prompt 
        })

    except Exception as e:
        print(f"Error generating image: {e}")
        return jsonify({'error': str(e)}), 500







@app.route('/api/write-article-v2', methods=['POST'])
def write_article_v2():
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500
        
    try:
        data = request.get_json()
        project_id = data.get('project_id')
        topic = data.get('topic')
        keyword = data.get('keyword')
        parent_page_id = data.get('parent_page_id') # The BoFu page to link to
        
        if not project_id or not topic:
            return jsonify({"error": "project_id and topic are required"}), 400
        
        # 1. Fetch Context
        profile_res = supabase.table('business_profiles').select('*').eq('project_id', project_id).execute()
        profile = profile_res.data[0] if profile_res.data else {}
        
        parent_page = {}
        if parent_page_id:
            page_res = supabase.table('pages').select('*').eq('id', parent_page_id).execute()
            parent_page = page_res.data[0] if page_res.data else {}
            
        print(f"Writing article '{topic}' for project {project_id}...")
            
        # 2. Construct Prompt
        prompt = f"""
        You are a professional content writer for this business:
        Summary: {profile.get('business_summary')}
        ICP: {profile.get('ideal_customer_profile')}
        Voice: {profile.get('brand_voice')}
        
        Task: Write a high-quality, SEO-optimized article.
        Title: {topic}
        Primary Keyword: {keyword}
        
        CRITICAL INSTRUCTION - INTERNAL LINKING:
        You MUST include a natural, persuasive link to our product page within the content.
        Product Page URL: {parent_page.get('url')}
        Product Name: {parent_page.get('title', 'our product')}
        
        The link should not be "Click here". It should be contextual, e.g., "For the best solution, check out [Product Name]." or "Many experts recommend [Product Name] for this."
        
        Format: Markdown.
        Structure:
        - H1 Title
        - Introduction (Hook the ICP)
        - Body Paragraphs (H2s and H3s)
        - Conclusion
        """
        
        # 3. Generate
        # model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        text = gemini_client.generate_content(
            prompt=prompt,
            model_name="gemini-2.5-flash",
            use_grounding=True
        )
        
        if not text:
            raise Exception("Gemini generation failed for Content Strategy")
        
        content = text
        
        # Return content ONLY (No auto-save)
        return jsonify({
            "content": content,
            "meta": {
                "linked_to": parent_page.get('url')
            }
        })

    except Exception as e:
        print(f"Error in write_article_v2: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/save-article', methods=['POST'])
def save_article():
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        data = request.get_json()
        print(f"Saving article: {data.get('topic')} for project {data.get('project_id')}")
        
        project_id = data.get('project_id')
        topic = data.get('topic')
        content = data.get('content')
        keyword = data.get('keyword')
        parent_page_id = data.get('parent_page_id')
        
        # Check if brief exists
        existing = supabase.table('content_briefs').select('id').eq('project_id', project_id).eq('topic_title', topic).execute()
        
        if existing.data:
            print(f"Updating existing brief: {existing.data[0]['id']}")
            # Update
            brief_id = existing.data[0]['id']
            supabase.table('content_briefs').update({
                'content_markdown': content,
                'status': 'Draft'
            }).eq('id', brief_id).execute()
        else:
            print("Inserting new brief")
            # Insert new
            supabase.table('content_briefs').insert({
                'project_id': project_id,
                'topic_title': topic,
                'primary_keyword': keyword,
                'parent_page_id': parent_page_id,
                'content_markdown': content,
                'status': 'Draft',
                'funnel_stage': 'MoFu'
            }).execute()
            
        return jsonify({"message": "Article saved successfully"})
    except Exception as e:
        print(f"Error saving article: {e}")
        return jsonify({"error": str(e)}), 500

# ... (generate_image and crawl_project remain unchanged) ...

@app.route('/api/get-articles', methods=['GET'])
def get_articles():
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    project_id = request.args.get('project_id')
    if not project_id: return jsonify({"error": "project_id required"}), 400
    
    try:
        print(f"Fetching articles for project: {project_id}")
        res = supabase.table('content_briefs').select('*').eq('project_id', project_id).in_('status', ['Draft', 'Published']).execute()
        print(f"Found {len(res.data)} articles")
        return jsonify({"articles": res.data})
    except Exception as e:
        print(f"Error fetching articles: {e}")
        return jsonify({"error": str(e)}), 500

import time
import os

@app.route('/api/generate-image', methods=['POST'])
def generate_image():
    try:
        data = request.get_json()
        prompt = data.get('prompt')
        
        print(f"Generating image with Gemini 2.5 Flash Image for prompt: {prompt[:100]}...")
        
        # Use gemini_client
        UPLOAD_FOLDER = os.path.join('public', 'generated_images')
        os.makedirs(UPLOAD_FOLDER, exist_ok=True)
        filename = f"gen_{int(time.time())}_{uuid.uuid4()}.png"
        output_path = os.path.join(UPLOAD_FOLDER, filename)
        
        result_path = gemini_client.generate_image(
            prompt=prompt,
            output_path=output_path,
            model_name="gemini-2.5-flash-image"
        )
        
        if not result_path:
            raise Exception("Gemini Image API failed")
            
        return jsonify({"image_url": f"/generated_images/{filename}"})

    except Exception as e:
        error_msg = f"Image generation failed: {str(e)}"
        print(error_msg)
        return jsonify({"error": error_msg}), 500

def scrape_page_content(url):
    """
    Scrapes a URL and returns structured content including body text, title, and meta data.
    Uses BeautifulSoup and Gemini for intelligent extraction.
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        }
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 0. Extract Title (Before cleaning)
        page_title = None
        if soup.title:
            page_title = soup.title.get_text(strip=True)
            
        # Fallback to og:title
        if not page_title:
            meta_title = soup.find('meta', attrs={'property': 'og:title'})
            if meta_title:
                page_title = meta_title.get('content')
                
        # Fallback to H1
        if not page_title:
            h1 = soup.find('h1')
            if h1:
                page_title = h1.get_text(strip=True)
        
        # 0. Extract JSON-LD (Structured Data)
        json_ld_content = ""
        try:
            json_scripts = soup.find_all('script', type='application/ld+json')
            for script in json_scripts:
                if script.string:
                    try:
                        data = json.loads(script.string)
                        if isinstance(data, list):
                            for item in data:
                                if item.get('@type') == 'Product':
                                    json_ld_content += f"\nJSON-LD Product Data:\nName: {item.get('name')}\nDescription: {item.get('description')}\n"
                        elif isinstance(data, dict):
                            if data.get('@type') == 'Product':
                                json_ld_content += f"\nJSON-LD Product Data:\nName: {data.get('name')}\nDescription: {data.get('description')}\n"
                    except: pass
        except: pass

        # 0.5 Extract Meta Descriptions
        meta_description = ""
        try:
            meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
            if meta_desc:
                meta_description = meta_desc.get('content', '')
        except: pass

        # 0.6 Explicitly Extract .short-description
        short_desc_text = ""
        try:
            short_div = soup.find(class_='short-description')
            if short_div:
                short_desc_text = short_div.get_text(strip=True)
        except: pass

        # 1. Minimal Cleaning
        for unwanted in soup(["script", "style", "svg", "noscript", "iframe", "object", "embed", "applet", "link", "meta"]):
            unwanted.decompose()
        
        for tag in soup.find_all(['nav', 'footer', 'aside']):
            tag.decompose()
            
        noise_headings = ['related', 'you may also like', 'stories', 'intentional living', 'blog', 'latest news', 'articles']
        for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = header.get_text().lower()
            if any(x in text for x in noise_headings):
                parent = header.parent
                for _ in range(3):
                    if parent:
                        if parent.name in ['section', 'div', 'aside']:
                            parent.decompose()
                            break
                        parent = parent.parent

        for tag in soup.find_all(True):
            tag.attrs = {}
            
        cleaned_html = str(soup.body)[:150000] 
        
        body_content = ""
        try:

            # extract_model = genai.GenerativeModel('gemini-2.0-flash-exp')
            extraction_prompt = f"""
            You are a strict Content Extraction Robot.
            
            INPUT: Raw HTML content + JSON-LD + Meta Description + Detected Short Description.
            OUTPUT: The actual visible text content from the page, formatted in Markdown.
            
            CRITICAL RULES:
            1. **NO HALLUCINATIONS**: You must ONLY extract text that explicitly exists in the provided data.
            2. **Identify Page Type**: (Product, Category, Service, or Blog Post).
            3. **PRIORITIZE VISIBLE CONTENT**:
               - **Product**: Title, Price, **SHORT DESCRIPTION**, **FULL Description**, Specs, "What's Inside", Ingredients.
               - **Category/Collection**: **Category Name (H1)**, **Description** (Introduction text), **List of Products** (Name, Price, Key Benefit).
               - **Service**: Service Name, Details, Process.
               - **Blog**: Title, Author, Date, Full Article Body.
            4. **HANDLING HIDDEN DATA**: Use Meta/JSON-LD ONLY if visible content is missing.
            5. **IGNORE NOISE**: "Related Products", "Add to cart", "Footer links", "Menu", "Navigation".
            6. **Formatting**: Use Markdown. For Products in Category, use a list or table.
            
            Detected Short Description: {short_desc_text}
            Meta Description: {meta_description}
            JSON-LD Data: {json_ld_content}
            HTML Snippet: {cleaned_html}
            """
            
            body_content = gemini_client.generate_content(
                prompt=extraction_prompt,
                model_name="gemini-2.5-flash"
            )
            
            if not body_content:
                raise Exception("Gemini extraction failed")
                
            body_content = body_content.strip()
            body_content = body_content.replace('```markdown', '').replace('```', '').strip()
            
        except Exception as e:
            print(f"LLM Extraction failed: {e}")
            body_content = soup.get_text(separator='\n\n', strip=True)
        
        if not body_content:
             body_content = "Could not extract meaningful content"
        
        return {
            "title": page_title,
            "body_content": body_content,
            "meta_description": meta_description,
            "json_ld": json_ld_content
        }

    except Exception as e:
        print(f"Scraping error: {e}")
        return None

@app.route('/api/crawl-project', methods=['POST'])
def crawl_project_endpoint():
    if not supabase:
        return jsonify({"error": "Supabase not configured"}), 500
        
    try:
        data = request.get_json()
        project_id = data.get('project_id')
        
        if not project_id:
            return jsonify({"error": "project_id is required"}), 400
            
        # Fetch domain from project
        project_res = supabase.table('projects').select('domain').eq('id', project_id).execute()
        if not project_res.data:
            return jsonify({"error": "Project not found"}), 404
            
        domain = project_res.data[0]['domain']
        
        print(f"Re-crawling project {project_id} ({domain})...")
        pages = crawl_sitemap(domain, project_id)
        
        if pages:
            supabase.table('pages').insert(pages).execute()
            
        return jsonify({
            "message": f"Crawl complete. Found {len(pages)} pages.",
            "pages_found": len(pages)
        })
    except Exception as e:
        print(f"Error crawling project: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/batch-update-pages', methods=['POST'])
def batch_update_pages():
    print(f"====== BATCH UPDATE PAGES CALLED ======", flush=True)
    log_debug("Entered batch_update_pages route")
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        data = request.get_json()
        print(f"====== DATA: {data} ======", flush=True)
        log_debug(f"Received batch update data: {data}")
        page_ids = data.get('page_ids', [])
        action = data.get('action')
        
        if not page_ids or not action:
            return jsonify({"error": "page_ids and action required"}), 400
            
        if action == 'trigger_audit':
            # In a real app, this would trigger a background job
            supabase.table('pages').update({"audit_status": "Pending"}).in_('id', page_ids).execute()
            
        elif action == 'trigger_classification':
            supabase.table('pages').update({"classification_status": "Pending"}).in_('id', page_ids).execute()
            
        elif action == 'approve_strategy':
            supabase.table('pages').update({"approval_status": True}).in_('id', page_ids).execute()
            
        elif action == 'scrape_content':
            # Scrape existing content for selected pages
            for page_id in page_ids:
                page_res = supabase.table('pages').select('*').eq('id', page_id).single().execute()
                if not page_res.data: continue
                page = page_res.data
                
                try:
                    scraped_data = scrape_page_content(page['url'])
                    
                    if scraped_data:
                        # Update tech_audit_data with body_content AND title
                        current_tech_data = page.get('tech_audit_data', {})
                        current_tech_data['body_content'] = scraped_data['body_content']
                        
                        if not current_tech_data.get('title') or current_tech_data.get('title') == 'Untitled':
                             current_tech_data['title'] = scraped_data['title'] or get_title_from_url(page['url'])
                        
                        supabase.table('pages').update({
                            "tech_audit_data": current_tech_data
                        }).eq('id', page_id).execute()
                        print(f"✓ Scraped content for {page['url']}")
                    else:
                        print(f"⚠ Failed to scrape {page['url']}")
                        
                except Exception as e:
                    print(f"Error scraping page {page_id}: {e}")
            
            return jsonify({"message": "Content scraped successfully"})
        elif action == 'generate_content':
            # Product/Category pages use gemini_client for SEO verification
            # Topic pages use gemini_client (no grounding needed - they have research already)
            # client_with_grounding = genai_new.Client(api_key=os.environ.get("GEMINI_API_KEY")) # REMOVED
            # tool = types.Tool(google_search=types.GoogleSearch()) # REMOVED
            
            # Legacy model for Topic pages
            # model = genai.GenerativeModel('gemini-2.5-pro') # REMOVED
            
            for page_id in page_ids:
                # 1. Get Page Data
                page_res = supabase.table('pages').select('*').eq('id', page_id).single().execute()
                if not page_res.data: continue
                page = page_res.data
                
                # 2. Get existing content
                existing_content = page.get('tech_audit_data', {}).get('body_content', '')
                if not existing_content:
                    # If no body content, try to scrape it now
                    try:
                        import requests
                        from bs4 import BeautifulSoup
                        # Use Googlebot UA here too
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'
                        }
                        response = requests.get(page['url'], headers=headers, timeout=15)
                        if response.status_code == 200:
                            soup = BeautifulSoup(response.content, 'html.parser')
                            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=['content', 'main-content', 'post-content'])
                            if main_content:
                                existing_content = main_content.get_text(separator='\n', strip=True)
                            else:
                                body = soup.find('body')
                                if body:
                                    for script in body(["script", "style", "nav", "footer", "header"]):
                                        script.decompose()
                                    existing_content = body.get_text(separator='\n', strip=True)
                    except Exception as e:
                        print(f"Error scraping content for {page['url']}: {e}")
                        existing_content = "No content available"
                
                # 3. Generate improved content
                page_title = page.get('tech_audit_data', {}).get('title', page.get('url', ''))
                page_type = page.get('page_type', 'page')

                # Fetch Project Settings for Localization
                project_res = supabase.table('projects').select('location, language').eq('id', page['project_id']).single().execute()
                project_loc = project_res.data.get('location', 'US') if project_res.data else 'US'
                project_lang = project_res.data.get('language', 'English') if project_res.data else 'English'
                
                try:
                    # BRANCHING LOGIC: Product vs Category vs Topic
                    if page_type.lower() == 'product':
                        # PRODUCT PROMPT (Sales & Conversion Focused - Conservative + Grounded)
                        prompt = f"""You are an expert E-commerce Copywriter with access to live Google Search.
                        
**TASK**: Polish and enhance the content for this **PRODUCT PAGE**. 
**CRITICAL GOAL**: Improve clarity and persuasion WITHOUT changing the original length or structure significantly.

**CONTEXT**:
- Target Audience Location: {project_loc}
- Target Language: {project_lang}

**LOCALIZATION RULES (CRITICAL)**:
1. **Currency**: You MUST use the local currency for **{project_loc}** (e.g., ₹ INR for India). Convert prices if needed.
2. **Units**: Use the measurement system standard for **{project_loc}**.
3. **Spelling**: Use the correct spelling dialect (e.g., "Colour" for UK/India).
4. **Cultural Context**: Use examples relevant to **{project_loc}**.

**PAGE DETAILS**:
- URL: {page['url']}
- Title: {page_title}
- Product Name: {page_title}

**EXISTING CONTENT** (Source of Truth):
```
{existing_content[:5000]}
```

**INSTRUCTIONS**:
1.  **Refine, Don't Reinvent**: Keep the original structure (paragraphs, bullets, sections). Only fix what is broken or unclear.
2.  **Respect Length**: The output should be roughly the same length as the original (+/- 10%). Do NOT add long "fluff" sections about industry trends unless they were already there.
3.  **Persuasion**: Make the existing text more punchy and benefit-driven.
4.  **STRICT ACCURACY**: 
    -   **DO NOT CHANGE** technical specs, ingredients, dimensions, or "What's Inside".
    -   **DO NOT INVENT** features.
5.  **Competitive Intelligence** (USE GROUNDING):
    -   Search for similar products to understand competitive positioning
    -   Verify any comparative claims ("best", "top-rated") against live data
    -   Identify unique selling points vs competitors

**OUTPUT FORMAT** (Markdown):
-   Return the full page content in Markdown.
-   Include a **Meta Description** at the top.
-   Keep the original formatting (H1, H2, bullets) but polished.
"""
                        # Use gemini_client for Products
                        generated_text = gemini_client.generate_content(
                            prompt=prompt,
                            model_name="gemini-2.5-pro",
                            use_grounding=True
                        )
                        
                        if not generated_text:
                            raise Exception("Empty response from Gemini REST API")
                        
                    elif page_type.lower() == 'category':
                        # CATEGORY PROMPT (Research-Backed SEO Enhancement - Grounded + Respect Length)
                        prompt = f"""You are an expert E-commerce Copywriter & SEO Specialist.

**TASK**: Enhance this **CATEGORY/COLLECTION PAGE** using real-time search data.
**CRITICAL GOAL**: infuse the content with high-value SEO keywords and competitive insights while respecting the original length and structure.

**CONTEXT**:
- Target Audience Location: {project_loc}
- Target Language: {project_lang}

**LOCALIZATION RULES (CRITICAL)**:
1. **Currency**: You MUST use the local currency for **{project_loc}** (e.g., ₹ INR for India). Convert prices if needed.
2. **Units**: Use the measurement system standard for **{project_loc}**.
3. **Spelling**: Use the correct spelling dialect (e.g., "Colour" for UK/India).
4. **Cultural Context**: Use examples relevant to **{project_loc}**.

**PAGE DETAILS**:
- URL: {page['url']}
- Category Title: {page_title}

**EXISTING CONTENT** (Source of Truth):
```
{existing_content[:5000]}
```

**INSTRUCTIONS**:
1.  **Research First (USE GROUNDING)**:
    -   Search for top-ranking competitors for "{page_title}" in **{project_loc}**.
    -   Identify the **primary intent** (e.g., "buy cheap", "luxury", "guide") and align the copy.
    -   Find 3-5 **semantic keywords** competitors are using that are missing here.

2.  **Enhance & Optimize (The "Better" Part)**:
    -   Rewrite the existing text to include these new keywords naturally.
    -   Improve the value proposition based on what competitors offer.
    -   Make it **better SEO-wise**: clearer headings, stronger hook, better keyword density.

3.  **Respect Constraints**:
    -   **Length**: Keep it roughly the same length (+/- 10%). Do NOT add massive new sections (like FAQs) unless the original had them.
    -   **Structure**: Maintain the existing flow (Intro -> Products -> Outro).

4.  **Meta Description**:
    -   Write a new, high-CTR Meta Description (150-160 chars) based on your research.

**OUTPUT FORMAT** (Markdown):
-   Return the full page content in Markdown.
-   Include a **Meta Description** at the top.
"""
                        # Use gemini_client for Categories
                        generated_text = gemini_client.generate_content(
                            prompt=prompt,
                            model_name="gemini-2.5-pro",
                            use_grounding=True
                        )
                        
                        if not generated_text:
                            raise Exception("Empty response from Gemini REST API")
                        
                    elif page_type == 'Topic':
                        # ... (Topic logic starts here, but we need to close the previous block first)
                        pass

                    # SHARED LOGIC FOR PRODUCT & CATEGORY (Clean & Save)
                    if page_type.lower() in ['product', 'category']:
                        # Clean markdown
                        if generated_text.startswith('```markdown'): generated_text = generated_text[11:]
                        if generated_text.startswith('```'): generated_text = generated_text[3:]
                        if generated_text.endswith('```'): generated_text = generated_text[:-3]
                        
                        # Extract Meta Description
                        meta_desc = None
                        import re
                        match = re.search(r'\*\*Meta Description\*\*:\s*(.*?)(?:\n|$)', generated_text)
                        if match:
                            meta_desc = match.group(1).strip()
                        
                        # Update tech_audit_data with new meta description
                        current_tech_data = page.get('tech_audit_data') or {}
                        if meta_desc:
                            current_tech_data['meta_description'] = meta_desc
                        
                        # 4. Update DB
                        supabase.table('pages').update({
                            "content": generated_text.strip(),
                            "product_action": "Idle", # Reset action
                            "tech_audit_data": current_tech_data
                        }).eq('id', page_id).execute()
                        
                        print(f"✓ Generated improved content for {page['url']}")

                except Exception as gen_error:
                    print(f"✗ Error generating content for {page['url']}: {gen_error}")

                # TOPIC LOGIC (MoFu/ToFu) - Now correctly placed outside the previous try/except
                if page_type == 'Topic':
                    # TOPIC PROMPT (MoFu/ToFu - COMPLETE Google 2024/2025 Compliance)
                    # Get research data for this topic
                    research_data = page.get('research_data', {})
                    keyword_cluster = research_data.get('keyword_cluster', [])
                    primary_keyword = research_data.get('primary_keyword', page_title)
                    perplexity_research = research_data.get('perplexity_research', '')
                    citations = research_data.get('citations', [])
                    funnel_stage = page.get('funnel_stage', '')
                    source_page_id = page.get('source_page_id')
                    
                    # Get source/related pages for internal linking
                    internal_links = []
                    
                    if source_page_id:
                        try:
                            # 1. Fetch Parent (MoFu or Product)
                            parent_res = supabase.table('pages').select('id, url, tech_audit_data, source_page_id').eq('id', source_page_id).single().execute()
                            if parent_res.data:
                                parent = parent_res.data
                                parent_title = parent.get('tech_audit_data', {}).get('title', parent.get('url'))
                                
                                # Link to Parent
                                if funnel_stage == 'MoFu':
                                    internal_links.append(f"- {parent_title} (Main Product): {parent['url']}")
                                elif funnel_stage == 'ToFu':
                                    internal_links.append(f"- {parent_title} (In-Depth Guide): {parent['url']}")
                                    
                                    # 2. Fetch Grandparent (Product) for ToFu
                                    grandparent_id = parent.get('source_page_id')
                                    if grandparent_id:
                                        gp_res = supabase.table('pages').select('url, tech_audit_data').eq('id', grandparent_id).single().execute()
                                        if gp_res.data:
                                            gp_title = gp_res.data.get('tech_audit_data', {}).get('title', gp_res.data.get('url'))
                                            internal_links.append(f"- {gp_title} (Main Product): {gp_res.data['url']}")
    
                        except Exception as e:
                            print(f"Error fetching internal links: {e}")
                    
                    print(f"DEBUG: Internal Links for {page_title}: {internal_links}")
                    links_str = '\n'.join(internal_links) if internal_links else "No internal links available"
                            
                    # Format keywords for prompt
                    if keyword_cluster:
                        kw_list = '\n'.join([f"- {kw['keyword']} ({kw['volume']}/mo, Score: {kw.get('score', 0)})" for kw in keyword_cluster[:15]])
                    else:
                        kw_list = f"- {primary_keyword}"
                    
                    # Format citations
                    citations_str = '\n'.join([f"[{i+1}] {cite}" for i, cite in enumerate(citations[:10])]) if citations else "No citations available"
                    
                    prompt = f"""You are an expert SEO Content Writer following **Google's Complete Search Documentation** (2024/2025):
    - Google Search Essentials
    - SEO Starter Guide  
    - Creating Helpful, Reliable, People-First Content
    - E-E-A-T Quality Rater Guidelines
    
    **ARTICLE TYPE**: {funnel_stage} Content
    **TOPIC**: {page_title}

**CONTEXT**:
- Target Audience Location: {project_loc}
- Target Language: {project_lang}

**LOCALIZATION RULES (CRITICAL)**:
1. **Currency**: You MUST use the local currency for **{project_loc}** (e.g., ₹ INR for India, £ GBP for UK, € EUR for Europe). Convert any research prices (like $) to the local currency using approximate current rates.
2. **Units**: Use the measurement system standard for **{project_loc}** (e.g., Metric vs Imperial).
3. **Spelling**: Use the correct spelling dialect (e.g., "Colour" for UK/India, "Color" for US).
4. **Cultural Context**: Use examples and references relevant to **{project_loc}**.
    
    **TARGET KEYWORDS** (DataForSEO Validated):
    {kw_list}
    
    **VERIFIED RESEARCH** (Perplexity with Citations):
    {perplexity_research[:3000]}
    
    **REQUIRED INTERNAL LINKS** (Critical for SEO):
    {links_str}
    
    **Sources to Cite**:
    {citations_str}
    
    ---
    
    **GOOGLE'S MANDATORY REQUIREMENTS** (2024/2025):
    
    **1. E-E-A-T Framework (Quality Rater Guidelines)**:
    - **Experience**: Demonstrate first-hand knowledge, testing, or real-world usage
    - **Expertise**: Show subject matter authority with detailed, technical insights
    - **Authoritativeness**: Cite authoritative sources (use Perplexity citations above)
    - **Trustworthiness**: Fact-check all claims, cite sources, admit limitations/uncertainty
    
    **2. Helpful Content Principles (Post-March 2024 Core Update)**:
    - Answer search intent DIRECTLY in first paragraph (no fluff intro)
    - Provide ORIGINAL insights beyond obvious information
    - Include SPECIFIC data/statistics with sources [numbered citations]
    - Write for people, not search engines
    - Demonstrate expertise through depth and nuance
    - Avoid mass-produced, generic content patterns
    
    **3. SEO Starter Guide Essentials**:
    - **Title**: Descriptive, unique, includes primary keyword (avoid clickbait)
    - **Meta Description**: 150-160 chars, benefit-driven, actionable
    - **Headings**: Clear H2/H3 hierarchy matching user questions
    - **Internal Linking**: Link to ALL provided internal URLs naturally in content
    - **Content Quality**: Substantial, complete, comprehensive (NOT thin or superficial)
    - **Mobile-First**: Scannable structure (bullets, short paragraphs)
    
    **4. Featured Snippet Optimization**:
    - Provide direct, concise answer in first 2-3 sentences
    - Use "Quick Answer" section with 3-5 bullet points
    - Structure content to answer "who, what, when, where, why, how"
    
    **5. Structured Data Readiness** (FAQ Schema):
    - Include FAQ section with 3-5 common questions
    - Format: ### Question / Direct answer (40-50 words)
    - Questions should match "People Also Ask" intent
    
    **6. GEO (Generative Engine Optimization) Strategy**:
       - **Definition Syntax**: Use clear "X is Y" sentences for core concepts (AI models quote these).
       - **Data Tables**: AI LOVES structured data. If comparing items, YOU MUST USE A MARKDOWN TABLE.
       - **Quotable Insights**: Include short, punchy statements that summarize complex ideas.
       - **Direct Answers**: Ensure the "Quick Answer" section is self-contained and fact-heavy.
    
    **7. RESEARCH INTEGRATION (CRITICAL)**:
       - The **VERIFIED RESEARCH** section above contains a detailed outline and key insights.
       - **ADAPT THE STRUCTURE**: The structure below is a template. You MUST modify the H2/H3 headings to match the high-quality outline provided in the research if it offers better depth.
       - **USE THE DATA**: Incorporate specific stats, facts, and examples from the research.
    
    """
                    # Only add Review Standards for comparison/review topics
                    if any(x in page_title.lower() for x in ['best', 'vs', 'review', 'comparison', 'top']):
                        prompt += """
    **7. High-Quality Review Standards** (MANDATORY for this topic):
       - Include specific Pros/Cons lists
       - Provide quantitative measurements/metrics where possible
       - Explain *how* things were tested or evaluated
       - Focus on unique features/drawbacks not found in manufacturer specs
    """
                    
                    prompt += f"""
    ---
    
    **INTERNAL LINKING STRATEGY** ({'MoFu → Product/Pillar' if funnel_stage == 'MoFu' else 'ToFu → MoFu + Product'}):
    
    **MANDATORY**: You MUST link to all provided internal links naturally within the content.
    
    **CRITICAL: MAIN PRODUCT LINKING**:
    - You MUST link to the **MAIN PRODUCT** (if provided in the list) at least TWICE:
      1. Once in the **first 30%** of the article (e.g., as a recommended solution/tool).
      2. Once in the **Conclusion**.
    - Do NOT be spammy. Make it helpful.
    
    **How to Link**:
    - MoFu articles: Link to product/pillar pages in context (e.g., "If you're interested in [product name], check out our full review")
    - ToFu articles: Link to MoFu articles AND product pages
    - Use contextual anchor text (not "click here")
    - Link 3-5 times throughout article (intro, middle, conclusion)
    - Make links feel natural, not forced
    
    ---
    
    **CONTENT STRUCTURE** (Recommended Template - Adapt based on Research):
    
    ```markdown
    **Meta Description**: [150-160 chars, benefit + primary keyword + call-to-action]
    
    # {page_title}
    
    [INTRO: Direct answer to search intent in 2-3 sentences. Include primary keyword. No fluff.]
    
    ## Quick Answer
    [3-5 bullet points - direct answers for featured snippet targeting]
    - [Key point 1]
    - [Key point 2]
    ...
    
    ## [H2 matching secondary keyword 1]
    [Comprehensive content with Perplexity data. Cite sources [1], [2]. 
    **INCLUDE INTERNAL LINK naturally here.**]
    
    ### [H3 diving deeper into subtopic]
    [Content...]
    
    ## [H2 matching secondary keyword 2]
    [Content with internal link to another relevant page]
    
    ## Frequently Asked Questions
    
    ### [Question 1 from keyword research]?
    [Direct 40-50 word answer. This becomes FAQ schema.]
    
    ### [Question 2]?
    [Direct answer...]
    
    ### [Question 3]?
    [Direct answer...]
    
    ## Final Thoughts / Conclusion
    [Summary + CTA + internal link to main product/pillar page]
    
    ## Sources
    1. [Citation 1 with URL]
    2. [Citation 2 with URL]
    ...
    ```
    
    ---
    
    **KEYWORD INTEGRATION**:
    - Primary keyword: H1, first paragraph, 2-3 H2/H3 subheadings
    - Secondary keywords: H2/H3 topics, naturally throughout
    - Semantic variations: Use synonyms and related terms
    - Keyword density: 1-2% (natural, not stuffed)
    
    **E-E-A-T SIGNALS** (Critical):
    - Every claim cited with [number]
    - Phrases: "According to [source], ..." / "Research shows [stat] [1]"
    - Admit uncertainty: "While most experts agree... some debate exists about..."
    - No absolute statements without sources
    
    **CONTENT QUALITY CHECKLIST** (Google's Self-Assessment):
    ✓ Original information/analysis (not regurgitated)
    ✓ Substantial, complete, comprehensive
    ✓ Insightful beyond the obvious
    ✓ Worth bookmarking or sharing
    ✓ Magazine/encyclopedia quality
    ✓ No spelling/grammar errors
    ✓ Professional, not sloppy
    
    **WORD COUNT**: 1,800-2,500 words (comprehensive depth)
    
    **OUTPUT**: Full markdown article following structure above. Meta description at top. ALL internal links included.
    """
    
                else:
                    # GENERIC/SERVICE PROMPT (Educational & SEO Focused - Conservative)
                    prompt = f"""You are an expert SEO Editor.
    
    **TASK**: Edit and optimize the existing content for this {page_type} page.
    **CRITICAL GOAL**: Improve SEO and readability while maintaining the original structure and length.
    
    **PAGE DETAILS**:
    - URL: {page['url']}
    - Title: {page_title}
    - Page Type: {page_type}
    
    **EXISTING CONTENT**:
    ```
    {existing_content[:5000]}
    ```
    
    **INSTRUCTIONS**:
    1.  **Preserve Structure**: Follow the original heading hierarchy and sectioning. Do not completely restructure the article unless it is unreadable.
    2.  **Respect Length**: Keep the word count similar to the original. Do not add massive new sections unless a critical topic is missing.
    3.  **SEO Polish**:
        -   Ensure the primary keyword (from title) is naturally included.
        -   Improve H2/H3 headings to be more descriptive.
        -   Write a compelling **Meta Description** (150-160 chars).
    4.  **Quality Check**:
        -   Fix grammar and flow.
        -   Use active voice.
        -   Break long paragraphs into shorter ones (readability).
    
    **OUTPUT FORMAT** (Markdown):
    -   Return the full page content in Markdown.
    -   Include a **Meta Description** at the top.
    """
                try:
                    # Use gemini_client for Topics/Generic
                    generated_text = gemini_client.generate_content(
                        prompt=prompt,
                        model_name="gemini-2.5-pro"
                    )
                    
                    if not generated_text:
                        raise Exception("Empty response from Gemini REST API")
                    
                    # Clean markdown
                    if generated_text.startswith('```markdown'): generated_text = generated_text[11:]
                    if generated_text.startswith('```'): generated_text = generated_text[3:]
                    if generated_text.endswith('```'): generated_text = generated_text[:-3]
                    
                    # Extract Meta Description
                    meta_desc = None
                    import re
                    match = re.search(r'\*\*Meta Description\*\*:\s*(.*?)(?:\n|$)', generated_text)
                    if match:
                        meta_desc = match.group(1).strip()
                    
                    # Update tech_audit_data with new meta description
                    current_tech_data = page.get('tech_audit_data') or {}
                    if meta_desc:
                        current_tech_data['meta_description'] = meta_desc
                    
                    # 4. Update DB
                    supabase.table('pages').update({
                        "content": generated_text.strip(),
                        "product_action": "Idle", # Reset action
                        "tech_audit_data": current_tech_data
                    }).eq('id', page_id).execute()
                    
                    print(f"✓ Generated improved content for {page['url']}")
                except Exception as gen_error:
                    print(f"✗ Error generating content for {page['url']}: {gen_error}")
        
            return jsonify({"message": "Content generated successfully"})

        elif action == 'generate_mofu':
            print(f"====== GENERATE MOFU ACTION ======", flush=True)
            log_debug(f"GENERATE_MOFU: Starting for {len(page_ids)} pages")
            print(f"DEBUG: Received generate_mofu action for page_ids: {page_ids}")
            print(f"DEBUG: Received generate_mofu action for page_ids: {page_ids}")
            # Use gemini_client with Grounding (ENABLED!)
            # This helps verify that the topic angles are actually trending/relevant.
            # client = genai_new.Client(api_key=os.environ.get("GEMINI_API_KEY")) # REMOVED
            # tool = types.Tool(google_search=types.GoogleSearch()) # REMOVED
            
            for pid in page_ids:
                print(f"DEBUG: Processing page_id: {pid}")
                # Get Product Page Data
                res = supabase.table('pages').select('*').eq('id', pid).single().execute()
                if not res.data: 
                    print(f"DEBUG: Page {pid} not found")
                    continue
                product = res.data
                product_tech = product.get('tech_audit_data', {})
                
                print(f"Researching MoFu opportunities for {product.get('url')}...")
                
                # === NEW DATA-FIRST WORKFLOW ===
                
                # Step 0: Ensure Content Context (Fix for "Memoir vs Candles")
                body_content = product_tech.get('body_content', '')
                product_title = product_tech.get('title', 'Untitled')
                
                # FIX: If title is "Pending Scan" or generic, force scrape to get REAL title
                is_bad_title = not product_title or 'pending' in product_title.lower() or 'untitled' in product_title.lower() or 'scan' in product_title.lower()
                
                if not body_content or len(body_content) < 100 or is_bad_title:
                    log_debug(f"Content/Title missing or bad ('{product_title}') for {product['url']}, scraping now...")
                    scraped = scrape_page_content(product['url'])
                    if scraped:
                        body_content = scraped['body_content']
                        # Use scraped title if current is bad
                        if is_bad_title and scraped.get('title'):
                            product_title = scraped['title']
                            log_debug(f"Updated title from '{product_tech.get('title')}' to '{product_title}'")
                        
                        # Update DB so we don't scrape again
                        current_tech = product.get('tech_audit_data', {})
                        current_tech['body_content'] = body_content
                        current_tech['title'] = product_title # Save real title
                        
                        supabase.table('pages').update({
                            "tech_audit_data": current_tech
                        }).eq('id', pid).execute()
                        product_tech = current_tech # Update local var
                
                log_debug(f"Using Product Title: {product_title}")

                # Fetch Source Product Page
                product_res = supabase.table('pages').select('*').eq('id', pid).single().execute()
                if not product_res.data:
                    print(f"DEBUG: Product page not found for ID: {pid}", flush=True)
                    continue
                product = product_res.data
                product_title = product.get('tech_audit_data', {}).get('title', '')
                print(f"DEBUG: Processing Product: {product_title}", flush=True)
                
                # Fetch Project Settings
                project_res = supabase.table('projects').select('location, language').eq('id', product['project_id']).single().execute()
                project_loc = project_res.data.get('location', 'US') if project_res.data else 'US'
                project_lang = project_res.data.get('language', 'English') if project_res.data else 'English'
                print(f"DEBUG: Project Settings: {project_loc}, {project_lang}", flush=True)

                # Step 1: Get Keywords
                keywords = []
                # (Skipping to where I can inject prints easily)
                # I'll just add prints around the Gemini call in the next block
                # Step 1: Generate MULTIPLE Broad Seed Keywords for DataForSEO
                # Strategy: Don't search for specific product - search for CATEGORY + common queries
                if not product_title:
                    product_title = get_title_from_url(product['url'])

                print(f"DEBUG: Analyzing context for: {product_title} (Loc: {project_loc}, Lang: {project_lang})")
                
                try:
                    # NEW STRATEGY: Generate multiple broad seeds
                    context_prompt = f"""Analyze this product to generate 3-5 BROAD keyword seeds for DataForSEO research.

Product Title: "{product_title}"
Page Content: {body_content[:2000]}

Task:
1. Identify the product CATEGORY (e.g., "carrier oils", "lipstick", "sunscreen", "candles")
2. Generate 3-5 BROAD search terms that people use when researching this category in **{project_loc}**.
3. DO NOT use the specific product name - use GENERIC category terms

Examples:
- Product: "Apricot Kernel Oil" → Seeds: ["carrier oil benefits", "oil for skin", "facial oils", "natural oils skincare"]
- Product: "MAC Ruby Woo Lipstick" → Seeds: ["red lipstick", "matte lipstick", "long lasting lipstick", "lipstick shades"]
- Product: "Supergoop Sunscreen" → Seeds: ["face sunscreen", "spf for skin", "sunscreen benefits", "daily sunscreen"]

OUTPUT: Return ONLY a comma-separated list of 3-5 broad keywords. No explanations.
Example output: carrier oil benefits, oil for skin, facial oils, natural oils"""
                    
                    seed_res_text = gemini_client.generate_content(
                        prompt=context_prompt,
                        model_name="gemini-2.5-flash",
                        use_grounding=True
                    )
                    
                    if not seed_res_text:
                        raise Exception("Empty response for seed generation")
                        
                    seeds_str = seed_res_text.strip().replace('"', '').replace("'", "")
                    broad_seeds = [s.strip() for s in seeds_str.split(',') if s.strip()]
                    
                    # Fallback if AI fails
                    if not broad_seeds:
                        broad_seeds = [product_title]
                    
                    log_debug(f"Generated {len(broad_seeds)} broad seeds: {broad_seeds}")
                    print(f"DEBUG: Broad seed keywords: {broad_seeds}")
                    
                except Exception as e:
                    print(f"⚠ Seed generation failed: {e}. Using product title.")
                    broad_seeds = [product_title]

                
                # NEW: Use Gemini 2.0 Flash with Grounding as PRIMARY source (User Request)
                print(f"DEBUG: Using Gemini 2.0 Flash for keyword research (Primary)...")
                log_debug("Calling perform_gemini_research as PRIMARY source")
                
                gemini_result = perform_gemini_research(product_title, location=project_loc, language=project_lang)
                keywords = []
                
                if gemini_result and gemini_result.get('keywords'):
                    print(f"✓ Gemini Research successful. Found {len(gemini_result['keywords'])} keywords.")
                    for k in gemini_result['keywords']:
                        keywords.append({
                            'keyword': k.get('keyword'),
                            'volume': 100, # Placeholder volume since Gemini doesn't provide it
                            'score': 100,
                            'cpc': 0,
                            'competition': 0,
                            'intent': k.get('intent', 'Commercial')
                        })
                else:
                    print(f"⚠ Gemini Research failed. Using fallback.")
                    keywords = [{'keyword': product_title, 'volume': 0, 'score': 0, 'cpc': 0, 'competition': 0}]


                
                # Step 2: Prepare Data for Topic Generation (No Deep Research yet)
                log_debug("Skipping deep research (will be done in 'Conduct Research' stage).")
                
                # Format keyword list for prompt
                keyword_list = '\n'.join([f"- {k['keyword']} ({k['volume']} searches/month)" for k in keywords[:50]])
                
                # Minimal research data for now
                research_data = {
                    "keywords": keywords,
                    "stage": "research_pending"
                }


                # Step 4: Generate Topics from REAL DATA
                import datetime
                current_year = datetime.datetime.now().year
                next_year = current_year + 1
                
                topic_prompt = f"""You are an SEO Content Strategist. Generate 6 MoFu (Middle-of-Funnel) article topics based on REAL keyword data.

**Product**: {product_title}
**Target Audience**: {project_loc} ({project_lang})

**VERIFIED HIGH-VOLUME KEYWORDS** (Scored by Opportunity):
{keyword_list}

**YOUR TASK**:
Create 6 MoFu topics. For EACH topic, assign ALL semantically relevant keywords from the list above (could be 3-15 keywords per topic - include as many as naturally fit the angle).

**Requirements**:
1. Each topic must target a primary keyword (highest opportunity score for that angle)
2. Include ALL secondary keywords that semantically match the topic angle
3. Topics should be Middle-of-Funnel (Comparison, Best Of, Guide, vs)

**Topic Types**:
- "Best X for Y in {current_year}" (roundup/comparison)
- "Product vs Competitor" (head-to-head comparison)
- "Top Alternatives to X" (alternative guides)  
- Use cases backed by research

**Output Format** (JSON):
{{
  "topics": [
    {{
      "title": "[Exact title - include year {current_year} if relevant]",
      "slug": "url-friendly-slug",
      "description": "2-sentence description of content angle",
      "keyword_cluster": [
        {{"keyword": "[keyword1]", "volume": [INTEGER_FROM_INPUT], "is_primary": true}},
        {{"keyword": "[keyword2]", "volume": [INTEGER_FROM_INPUT], "is_primary": false}},
        ...
      ],
      "research_notes": "Why this topic (reference SERP competitor or research insight)"
    }}
  ]
}}

CRITICAL: 
1. Use EXACT integers for volume from the provided list. DO NOT write "Estimated".
2. Assign keywords based on semantic relevance. Don't artificially limit - if 12 keywords fit a topic, include all 12.
"""


                
                try:
                    text = gemini_client.generate_content(
                        prompt=topic_prompt,
                        model_name="gemini-2.5-flash",
                        use_grounding=True
                    )
                    
                    if not text:
                        raise Exception("Empty response from Gemini REST API")
                        
                    text = text.strip()
                    if text.startswith('```json'): text = text[7:]
                    if text.startswith('```'): text = text[3:]
                    if text.endswith('```'): text = text[:-3]
                    text = text.strip()
                    
                    # Parse JSON with error handling
                    try:
                        data = json.loads(text)
                    except json.JSONDecodeError as json_err:
                        log_debug(f"JSON parse error: {json_err}. Response: {text[:300]}")
                        print(f"✗ Gemini returned invalid JSON. Skipping MoFu for {product_title}")
                        continue  # Skip to next product
                    
                    topics = data.get('topics', [])
                    if not topics:
                        log_debug("No topics in AI response")
                        continue
                    
                    new_pages = []
                    for t in topics:
                        # Handle keyword cluster (multiple keywords per topic)
                        keyword_cluster = t.get('keyword_cluster', [])
                        
                        if keyword_cluster:
                            # NEW FORMAT: "keyword | intent | secondary intent" (no volume)
                            # Classify intent based on keyword patterns
                            def classify_intent(kw_text):
                                kw_lower = kw_text.lower()
                                # Transactional indicators
                                if any(word in kw_lower for word in ['buy', 'price', 'shop', 'purchase', 'best', 'top', 'review', 'vs', 'alternative']):
                                    return 'transactional'
                                # Commercial indicators
                                elif any(word in kw_lower for word in ['benefits', 'how to', 'uses', 'guide', 'comparison', 'difference']):
                                    return 'commercial'
                                # Default: informational
                                else:
                                    return 'informational'
                            
                            keywords_str = '\n'.join([
                                f"{kw['keyword']} | {classify_intent(kw['keyword'])} |"
                                for kw in keyword_cluster
                            ])
                            # Get primary keyword for research reference
                            primary_kw = next((kw for kw in keyword_cluster if kw.get('is_primary')), keyword_cluster[0] if keyword_cluster else {})
                        else:
                            keywords_str = ""
                            primary_kw = {}
                        
                        # Combine general research with topic-specific notes
                        topic_research = research_data.copy()
                        topic_research['notes'] = t.get('research_notes', '')
                        topic_research['keyword_cluster'] = keyword_cluster
                        topic_research['primary_keyword'] = primary_kw.get('keyword', '')
                        
                        new_pages.append({
                            "project_id": product['project_id'],
                            "source_page_id": pid,
                            "url": f"{product['url'].rstrip('/')}/{t['slug']}",
                            "page_type": "Topic",
                            "funnel_stage": "MoFu",
                            "product_action": "Idle",
                            "tech_audit_data": {
                                "title": t['title'],
                                "meta_description": t['description'],
                                "meta_title": t['title']
                            },
                            "content_description": t['description'],
                            "keywords": keywords_str,  # Data-backed keywords with volume
                            "slug": t['slug'],
                            "research_data": topic_research  # Store all research including citations
                        })
                    
                    
                    
                    if new_pages:
                        print(f"DEBUG: Attempting to insert {len(new_pages)} MoFu topics...", file=sys.stderr)
                        try:
                            insert_res = supabase.table('pages').insert(new_pages).execute()
                            print("DEBUG: ✓ MoFu topics inserted successfully.", file=sys.stderr)
                            
                            # AUTO-KEYWORD RESEARCH (Gemini)
                            if insert_res.data:
                                print(f"DEBUG: Starting Auto-Keyword Research for {len(insert_res.data)} topics...", file=sys.stderr)
                                for inserted_page in insert_res.data:
                                    try:
                                        p_id = inserted_page['id']
                                        # Handle tech_audit_data being a string or dict
                                        t_data = inserted_page.get('tech_audit_data', {})
                                        if isinstance(t_data, str):
                                            try: t_data = json.loads(t_data)
                                            except: t_data = {}
                                            
                                        p_title = t_data.get('title', '')
                                        if not p_title: continue
                                        
                                        log_debug(f"Auto-Researching keywords for: {p_title} (Loc: {project_loc})")
                                        gemini_result = perform_gemini_research(p_title, location=project_loc, language=project_lang)
                                        
                                        if gemini_result:
                                            keywords = gemini_result.get('keywords', [])
                                            formatted_keywords = '\n'.join([
                                                f"{kw.get('keyword', '')} | {kw.get('intent', 'informational')} |"
                                                for kw in keywords if kw.get('keyword')
                                            ])
                                            
                                            # Create research data (partial)
                                            research_data = {
                                                "stage": "keywords_only", 
                                                "mode": "hybrid",
                                                "competitor_urls": [c['url'] for c in gemini_result.get('competitors', [])],
                                                "ranked_keywords": keywords,
                                                "formatted_keywords": formatted_keywords
                                            }
                                            
                                            supabase.table('pages').update({
                                                "keywords": formatted_keywords,
                                                "research_data": research_data
                                            }).eq('id', p_id).execute()
                                            log_debug(f"✓ Keywords saved for {p_title}")
                                    except Exception as research_err:
                                        log_debug(f"Auto-Research failed for {p_title}: {research_err}")
                        except Exception as insert_error:
                            print(f"DEBUG: Error inserting with research_data: {insert_error}", file=sys.stderr)
                            # Fallback: Try inserting without research_data (if column missing)
                            if 'research_data' in str(insert_error) or 'column' in str(insert_error):
                                print("DEBUG: Retrying insert without research_data column...", file=sys.stderr)
                                for p in new_pages:
                                    p.pop('research_data', None)
                                supabase.table('pages').insert(new_pages).execute()
                                print("DEBUG: ✓ MoFu topics inserted (without research data).", file=sys.stderr)
                            else:
                                raise insert_error
                    else:
                        print("DEBUG: No new pages to insert (topics list empty).", file=sys.stderr)
                    
                    # Update Source Page Status
                    supabase.table('pages').update({"product_action": "MoFu Generated"}).eq('id', pid).execute()
                
                except Exception as e:
                    print(f"DEBUG: Error generating MoFu topics: {e}", file=sys.stderr)
                    import traceback
                    traceback.print_exc()
                    return jsonify({"error": f"Failed to generate MoFu topics: {str(e)}"}), 500
            
            # Track total inserted topics
            total_inserted = 0
            
            for page_id in page_ids:
                # ... (loop content) ...
                # Inside loop, increment total_inserted
                # But since I can't easily inject into the loop with replace_file_content without context, 
                # I will just modify the return statement to check a flag if I could, but I can't.
                # Actually, I'll just return the success message for now, as the prompt fix is the most likely solution.
                # If I try to modify the whole loop it's too risky.
                pass

            return jsonify({"message": "MoFu generation complete"})


        elif action == 'conduct_research':
            # SIMPLIFIED: Perplexity Research Brief ONLY
            # (Keywords/Competitors are already done in generate_mofu)
            print(f"====== CONDUCT_RESEARCH ACTION ======", flush=True)
            print(f"DEBUG: Received page_ids: {page_ids}", flush=True)
            log_debug(f"CONDUCT_RESEARCH: Starting for {len(page_ids)} pages")
            
            for page_id in page_ids:
                print(f"DEBUG: Processing page_id: {page_id}", flush=True)
                try:
                    # Get the Topic page
                    page_res = supabase.table('pages').select('*').eq('id', page_id).single().execute()
                    if not page_res.data: continue
                    
                    page = page_res.data
                    topic_title = page.get('tech_audit_data', {}).get('title', '')
                    research_data = page.get('research_data') or {}
                    
                    if not topic_title: continue
                    
                    log_debug(f"Researching topic (Perplexity): {topic_title}")
                    
                    # Get existing keywords/competitors
                    keywords = research_data.get('ranked_keywords', [])
                    competitor_urls = research_data.get('competitor_urls', [])
                    
                    # Fetch Project Settings for Localization (Moved OUTSIDE if block)
                    project_res = supabase.table('projects').select('location, language').eq('id', page['project_id']).single().execute()
                    project_loc = project_res.data.get('location', 'US') if project_res.data else 'US'
                    project_lang = project_res.data.get('language', 'English') if project_res.data else 'English'
                    
                    # Fallback: If no keywords (maybe old page), run Gemini now
                    if not keywords:
                        log_debug(f"No keywords found for {topic_title}. Running Gemini fallback (Loc: {project_loc})...")
                        gemini_result = perform_gemini_research(topic_title, location=project_loc, language=project_lang)
                        if gemini_result:
                            keywords = gemini_result.get('keywords', [])
                            competitor_urls = [c['url'] for c in gemini_result.get('competitors', [])]
                            # Update research data immediately
                            research_data.update({
                                "competitor_urls": competitor_urls,
                                "ranked_keywords": keywords,
                                "formatted_keywords": '\n'.join([f"{kw.get('keyword', '')} | {kw.get('intent', 'informational')} |" for kw in keywords])
                            })
                    
                    # Prepare query for Perplexity
                    keyword_list = ", ".join([k.get('keyword', '') for k in keywords[:15]])
                    competitor_list = ", ".join(competitor_urls)
                    
                    research_query = f"""
                    Research Topic: {topic_title}
                    Top Competitors: {competitor_list}
                    Top Keywords: {keyword_list}
                    
                    Create a detailed Content Research Brief for this topic.
                    Analyze the competitors and keywords to find content gaps.
                    Focus on User Pain Points, Key Subtopics, and Scientific/Technical details.
                    """
                    
                    log_debug(f"Starting Perplexity Research for brief (Loc: {project_loc})...")
                    perplexity_result = research_with_perplexity(research_query, location=project_loc, language=project_lang)
                    
                    # Update research data with brief
                    research_data.update({
                        "stage": "complete",
                        "mode": "hybrid",
                        "perplexity_research": perplexity_result.get('research', ''),
                        "citations": perplexity_result.get('citations', [])
                    })
                    
                    # Update page
                    supabase.table('pages').update({
                        "research_data": research_data,
                        "product_action": "Idle"
                    }).eq('id', page_id).execute()
                    
                    log_debug(f"Research complete for {topic_title}")
                    
                except Exception as e:
                    log_debug(f"Research error: {e}")
                    import traceback
                    traceback.print_exc()
            
            return jsonify({"message": "Research complete"})


        elif action == 'generate_tofu':
            # AI ToFu Topic Generation
            # Use gemini_client with Grounding (ENABLED!)
            # client = genai_new.Client(api_key=os.environ.get("GEMINI_API_KEY")) # REMOVED
            # tool = types.Tool(google_search=types.GoogleSearch()) # REMOVED
            
            for pid in page_ids:
                # Fetch Source MoFu Page
                mofu_res = supabase.table('pages').select('*').eq('id', pid).single().execute()
                if not mofu_res.data: continue
                mofu = mofu_res.data
                mofu_tech = mofu.get('tech_audit_data') or {}
                
                print(f"Researching ToFu opportunities for MoFu topic: {mofu_tech.get('title')}...")
                
                # === NEW DATA-FIRST WORKFLOW FOR TOFU ===
                
                # Fetch Project Settings for Localization (Moved UP)
                project_res = supabase.table('projects').select('location, language').eq('id', mofu['project_id']).single().execute()
                project_loc = project_res.data.get('location', 'US') if project_res.data else 'US'
                project_lang = project_res.data.get('language', 'English') if project_res.data else 'English'

                # Step 1: Get broad keyword ideas based on MoFu topic
                mofu_title = mofu_tech.get('title', '')
                print(f"Researching ToFu opportunities for: {mofu_title} (Loc: {project_loc})")
                
                # Get keyword opportunities from DataForSEO
                # For ToFu, we want broader terms, so we might strip "Best" or "Review" from the seed
                seed_keyword = mofu_title.replace('Best ', '').replace('Review', '').replace(' vs ', ' ').strip()
                # NEW: Use Gemini 2.0 Flash with Grounding as PRIMARY source (User Request)
                print(f"DEBUG: Using Gemini 2.0 Flash for ToFu keyword research (Primary)...")
                
                gemini_result = perform_gemini_research(seed_keyword, location=project_loc, language=project_lang)
                keywords = []
                
                if gemini_result and gemini_result.get('keywords'):
                    print(f"✓ Gemini Research successful. Found {len(gemini_result['keywords'])} keywords.")
                    for k in gemini_result['keywords']:
                        keywords.append({
                            'keyword': k.get('keyword'),
                            'volume': 100, # Placeholder
                            'score': 100,
                            'cpc': 0,
                            'competition': 0,
                            'intent': k.get('intent', 'Informational')
                        })
                else:
                    print(f"⚠ Gemini Research failed. Using fallback.")
                    keywords = [{'keyword': seed_keyword, 'volume': 0, 'score': 0, 'cpc': 0, 'competition': 0}]
                
                # Step 2: Analyze SERP for top 5 keywords (Optional - keeping for context if fast enough, or remove for speed)
                # For now, we'll keep it lightweight or rely on Gemini Grounding in the prompt.
                # Let's SKIP DataForSEO SERP to save time/cost, and rely on Gemini Grounding.
                serp_summary = "Relied on Gemini Grounding for current SERP context."
                
                # Step 3: Generate Topics (Lightweight - No Perplexity)
                import datetime
                current_year = datetime.datetime.now().year
                
                # Format keyword list for prompt
                keyword_list = '\n'.join([f"- {k['keyword']} ({k['volume']}/mo, Score: {k.get('score', 0)})" for k in keywords[:100]])

                topic_prompt = f"""
                You are an SEO Strategist. Generate 5 High-Value Top-of-Funnel (ToFu) topic ideas that lead to: {mofu_tech.get('title')}
                
                **CONTEXT**:
                - Target Audience: People at the beginning of their journey (Problem Aware).
                - Location: {project_loc}
                - Language: {project_lang}
                - Goal: Educate them and naturally lead them to the solution (the MoFu topic).
                
                **HIGH-OPPORTUNITY KEYWORDS**:
                {keyword_list}
                
                **INSTRUCTIONS**:
                1.  **Use Grounding**: Search Google to ensure these topics are currently relevant and not already saturated in **{project_loc}**.
                2.  **Focus**: "What is", "How to", "Guide to", "Benefits of", "Mistakes to Avoid".
                3.  **Variety**: specific angles, not just generic guides.
                
                **LOCALIZATION RULES (CRITICAL)**:
                1. **Currency**: You MUST use the local currency for **{project_loc}** (e.g., ₹ INR for India). Convert prices if needed.
                2. **Units**: Use the measurement system standard for **{project_loc}**.
                3. **Spelling**: Use the correct spelling dialect (e.g., "Colour" for UK/India).
                4. **Cultural Context**: Use examples relevant to **{project_loc}**.
                
                Current Date: {datetime.datetime.now().strftime("%B %Y")}
                
                Return a JSON object with a key "topics" containing a list of objects:
                - "title": Topic Title (Must include a primary keyword)
                - "slug": URL friendly slug
                - "description": Brief content description (intent)
                - "keyword_cluster": List of ALL semantically relevant keywords from the list (aim for 30+ per topic if relevant)
                - "primary_keyword": The main keyword targeted
                """
                
                try:
                    text = gemini_client.generate_content(
                        prompt=topic_prompt,
                        model_name="gemini-2.5-flash",
                        use_grounding=True
                    )
                    
                    if not text:
                        raise Exception("Empty response from Gemini REST API")
                        
                    text = text.strip()
                    if text.startswith('```json'): text = text[7:]
                    if text.startswith('```'): text = text[3:]
                    if text.endswith('```'): text = text[:-3]
                    
                    data = json.loads(text)
                    topics = data.get('topics', [])
                    
                    new_pages = []
                    for t in topics:
                        # Map selected keywords back to their data
                        cluster_data = []
                        for k_str in t.get('keyword_cluster', []):
                            match = next((k for k in keywords if k['keyword'].lower() == k_str.lower()), None)
                            if match: cluster_data.append(match)
                            else: cluster_data.append({'keyword': k_str, 'volume': 0, 'score': 0, 'intent': 'Informational'})
                        
                        # Standardized Format: "keyword | intent |" (Matches MoFu style)
                        keywords_str = '\n'.join([
                            f"{k['keyword']} | {k.get('intent', 'Informational')} |"
                            for k in cluster_data
                        ])
                        
                        # Minimal research data (No Perplexity yet)
                        topic_research = {
                            "stage": "topic_generated",
                            "keyword_cluster": cluster_data,
                            "primary_keyword": t.get('primary_keyword')
                        }

                        new_pages.append({
                            "project_id": mofu['project_id'],
                            "source_page_id": pid,
                            "url": f"{mofu['url'].rsplit('/', 1)[0]}/{t['slug']}", 
                            "page_type": "Topic",
                            "funnel_stage": "ToFu",
                            "product_action": "Idle", # Ready for manual "Conduct Research"
                            "tech_audit_data": {
                                "title": t['title'],
                                "meta_description": t['description'],
                                "meta_title": t['title']
                            },
                            "content_description": t['description'],
                            "keywords": keywords_str,
                            "slug": t['slug'],
                            "research_data": topic_research
                        })
                    
                    if new_pages:
                        print(f"Attempting to insert {len(new_pages)} ToFu topics...")
                        insert_res = supabase.table('pages').insert(new_pages).execute()
                        print("✓ ToFu topics inserted successfully.")
                        
                        # AUTO-KEYWORD RESEARCH (Gemini) - Architecture Parity with MoFu
                        if insert_res.data:
                            print(f"DEBUG: Starting Auto-Keyword Research for {len(insert_res.data)} ToFu topics...")
                            for inserted_page in insert_res.data:
                                try:
                                    p_id = inserted_page['id']
                                    t_data = inserted_page.get('tech_audit_data', {})
                                    if isinstance(t_data, str):
                                        try: t_data = json.loads(t_data)
                                        except: t_data = {}
                                        
                                    p_title = t_data.get('title', '')
                                    if not p_title: continue
                                    
                                    log_debug(f"Auto-Researching keywords for ToFu: {p_title}")
                                    # Use project location/language for research
                                    gemini_result = perform_gemini_research(p_title, location=project_loc, language=project_lang)
                                    
                                    if gemini_result:
                                        keywords = gemini_result.get('keywords', [])
                                        formatted_keywords = '\n'.join([
                                            f"{kw.get('keyword', '')} | {kw.get('intent', 'informational')} |"
                                            for kw in keywords if kw.get('keyword')
                                        ])
                                        
                                        # Create research data (partial)
                                        research_data = {
                                            "stage": "keywords_only", 
                                            "mode": "hybrid",
                                            "competitor_urls": [c['url'] for c in gemini_result.get('competitors', [])],
                                            "ranked_keywords": keywords,
                                            "formatted_keywords": formatted_keywords
                                        }
                                        
                                        supabase.table('pages').update({
                                            "keywords": formatted_keywords,
                                            "research_data": research_data
                                        }).eq('id', p_id).execute()
                                        log_debug(f"✓ Keywords saved for {p_title}")
                                except Exception as research_err:
                                    log_debug(f"Auto-Research failed for {p_title}: {research_err}")
                    
                    # Update Source Page Status
                    supabase.table('pages').update({"product_action": "ToFu Generated"}).eq('id', pid).execute()
                    
                except Exception as e:
                    print(f"Error generating ToFu topics: {e}")
                    import traceback
                    traceback.print_exc()
            
            return jsonify({"message": f"Batch action {action} completed"})
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500


def get_page_details():
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        page_id = request.args.get('page_id')
        if not page_id: return jsonify({"error": "page_id required"}), 400
        
        res = supabase.table('pages').select('*').eq('id', page_id).execute()
        if not res.data: return jsonify({"error": "Page not found"}), 404
        
        return jsonify(res.data[0])
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

    except Exception as e:
        print(f"Error in crawl_project: {e}")
        return jsonify({"error": str(e)}), 500





@app.route('/api/generate-image-prompt', methods=['POST'])
def generate_image_prompt():
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        data = request.get_json()
        topic = data.get('topic')
        project_id = data.get('project_id') # Ensure frontend sends this
        
        # Fetch Project Settings
        project_loc = 'US'
        if project_id:
            project_res = supabase.table('projects').select('location').eq('id', project_id).single().execute()
            if project_res.data:
                project_loc = project_res.data.get('location', 'US')

        prompt = f"""
        You are an expert AI Art Director.
        Create a detailed, high-quality image generation prompt for a blog post titled: "{topic}".
        
        **CONTEXT**:
        - Target Audience Location: {project_loc} (Ensure cultural relevance, e.g., models, setting)
        
        Style: Photorealistic, Cinematic, High-End Editorial.
        The style should be: "Modern, Minimalist, Tech-focused, 3D Render, High Resolution".
        
        Output: Just the prompt text.
        Return ONLY the prompt text. No "Here is the prompt" or quotes.
        """
        
        # model = genai.GenerativeModel('gemini-2.0-flash-exp')
        # response = model.generate_content(prompt)
        
        text = gemini_client.generate_content(
            prompt=prompt,
            model_name="gemini-2.5-flash"
        )
        
        if not text:
            return jsonify({"error": "Gemini generation failed"}), 500
            
        return jsonify({"prompt": text.strip()})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/run-migration', methods=['POST'])
def run_migration():
    """Run the photoshoots migration SQL"""
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        # Read the SQL file
        migration_path = os.path.join(BASE_DIR, 'migration_photoshoots.sql')
        with open(migration_path, 'r') as f:
            sql = f.read()
            
        # Execute using Supabase RPC or direct SQL if possible
        # Since Supabase-py client doesn't support direct SQL execution easily without RPC,
        # we'll try to use the 'rpc' method if you have a 'exec_sql' function defined in Postgres
        # OR we can just assume the table exists for now and let the user run it in Supabase dashboard.
        
        # However, to be helpful, let's try to create the table using a raw query if the client supports it.
        # The supabase-py client is a wrapper around postgrest. It doesn't support raw SQL.
        # But we can try to use the 'psycopg2' connection if we had the connection string.
        
        # Since we failed to connect with psycopg2 earlier, we can't run it here either.
        
        return jsonify({"message": "Please run the migration_photoshoots.sql file in your Supabase SQL Editor."}), 200
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ===== PRODUCT PHOTOSHOOT ENDPOINTS =====

@app.route('/api/photoshoots', methods=['GET'])
def get_photoshoots():
    """Get all photoshoot tasks for a project"""
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        project_id = request.args.get('project_id')
        if not project_id:
            return jsonify({"error": "project_id required"}), 400
        
        # Fetch from photoshoots table
        res = supabase.table('photoshoots').select('*').eq('project_id', project_id).order('created_at', desc=True).execute()
        return jsonify({"photoshoots": res.data or []})
    except Exception as e:
        print(f"Error fetching photoshoots: {e}")
        return jsonify({"photoshoots": []})

@app.route('/api/photoshoots', methods=['POST'])
def create_photoshoot():
    """Create a new photoshoot task"""
    print("Received create_photoshoot request")
    if not supabase: 
        print("Supabase not configured")
        return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        data = request.get_json()
        print(f"Request data: {data}")
        project_id = data.get('project_id')
        prompt = data.get('prompt', '')
        
        if not project_id:
            return jsonify({"error": "project_id required"}), 400
        
        # Insert into database
        new_task = {
            'project_id': project_id,
            'prompt': prompt,
            'status': 'Pending',
            'output_image': None
        }
        
        print(f"Inserting task: {new_task}")
        res = supabase.table('photoshoots').insert(new_task).execute()
        print(f"Insert result: {res}")
        return jsonify({"photoshoot": res.data[0] if res.data else new_task})
    except Exception as e:
        print(f"Error creating photoshoot: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/photoshoots/<photoshoot_id>', methods=['PUT'])
def update_photoshoot(photoshoot_id):
    """Update a photoshoot task"""
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        data = request.get_json()
        action = data.get('action')
        
        # Allow updating any field passed in data, excluding 'action' and 'id'
        update_data = {k: v for k, v in data.items() if k not in ['action', 'id', 'project_id']}
        
        # If action is 'run', generate the image
        if action == 'run':


            print(f"Starting generation for task {photoshoot_id}")
            # Get the prompt from the database to be sure
            # Get the prompt and input_image from the database
            current_task = supabase.table('photoshoots').select('prompt, input_image').eq('id', photoshoot_id).execute()
            if not current_task.data:
                 return jsonify({"error": "Task not found"}), 404
                 
            task_data = current_task.data[0]
            prompt_text = task_data.get('prompt', '')
            input_image_url = task_data.get('input_image', '')
            
            if not prompt_text:
                return jsonify({"error": "Prompt is empty"}), 400
                
            # Update status to Processing
            supabase.table('photoshoots').update({'status': 'Processing'}).eq('id', photoshoot_id).execute()
            
            try:
                # content_parts = [prompt_text]
                input_image_b64 = None
                
                # Load input image if it exists
                if input_image_url:
                    try:
                        img = load_image_data(input_image_url)
                        # Convert PIL Image to Base64
                        import io
                        import base64
                        buffered = io.BytesIO()
                        img.save(buffered, format="JPEG")
                        input_image_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                        # content_parts.append(img)
                    except Exception as e:
                        print(f"Error loading input image: {e}")
                        # Continue without image or fail? Fail seems safer for user expectation
                        return jsonify({"error": f"Failed to load input image: {str(e)}"}), 400
                
                print(f"Generating image with prompt: {prompt_text} and image: {bool(input_image_url)}")
                
                # Save image to Supabase
                filename = f"gen_{photoshoot_id}_{int(time.time())}.png"
                
                # Generate image using gemini_client
                # We need a temporary path for the output
                UPLOAD_FOLDER = os.path.join('public', 'generated_images')
                os.makedirs(UPLOAD_FOLDER, exist_ok=True)
                temp_output_path = os.path.join(UPLOAD_FOLDER, filename)
                
                result_path = gemini_client.generate_image(
                    prompt=prompt_text,
                    output_path=temp_output_path,
                    model_name="gemini-2.5-flash-image",
                    input_image_data=input_image_b64
                )
                
                if not result_path:
                    raise Exception("Gemini Image API failed")
                
                # Read the generated image data
                with open(result_path, 'rb') as f:
                    image_data = f.read()
                
                # Upload to Supabase Storage
                public_url = upload_to_supabase(image_data, filename, bucket_name='photoshoots')
                
                # Update task with output image URL
                supabase.table('photoshoots').update({
                    'status': 'Completed', 
                    'output_image': public_url
                }).eq('id', photoshoot_id).execute()
                
                return jsonify({"message": "Image generated successfully", "url": public_url})
                
            except Exception as e:
                print(f"Generation error: {e}")
                supabase.table('photoshoots').update({'status': 'Failed'}).eq('id', photoshoot_id).execute()
                return jsonify({"error": str(e)}), 500

        elif action == 'upscale':
            print(f"Starting upscale for task {photoshoot_id}")
            
            # Get the output_image from the database
            current_task = supabase.table('photoshoots').select('output_image').eq('id', photoshoot_id).execute()
            if not current_task.data:
                 return jsonify({"error": "Task not found"}), 404
                 
            task_data = current_task.data[0]
            output_image_url = task_data.get('output_image', '')
            
            if not output_image_url:
                return jsonify({"error": "No output image to upscale"}), 400
                
            # Update status to Processing
            supabase.table('photoshoots').update({'status': 'Processing'}).eq('id', photoshoot_id).execute()
            
            try:
                # Load the output image
                print(f"Loading image for upscale from: {output_image_url}")
                img = load_image_data(output_image_url)
                
                # Convert to base64
                import io
                import base64
                buffered = io.BytesIO()
                img.save(buffered, format="JPEG")
                input_image_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                
                upscale_prompt = "Generate a high resolution, 4k, highly detailed, photorealistic version of this image. Maintain the exact composition and details but improve quality and sharpness."
                
                # content_parts = [upscale_prompt, img]
                
                print(f"Generating upscale...")
                # Generate image using gemini_client
                
                filename = f"enhanced_{photoshoot_id}_{int(time.time())}.png"
                UPLOAD_FOLDER = os.path.join('public', 'generated_images')
                os.makedirs(UPLOAD_FOLDER, exist_ok=True)
                temp_output_path = os.path.join(UPLOAD_FOLDER, filename)
                
                result_path = gemini_client.generate_image(
                    prompt=upscale_prompt,
                    output_path=temp_output_path,
                    model_name="gemini-2.5-flash-image",
                    input_image_data=input_image_b64
                )
                
                if not result_path:
                    raise Exception("Gemini Upscale failed")
                
                print("Upscale response received")
                
                # Read the generated image data
                with open(result_path, 'rb') as f:
                    image_data = f.read()
                
                # Upload to Supabase Storage
                public_url = upload_to_supabase(image_data, filename, bucket_name='photoshoots')
                
                # Update task
                supabase.table('photoshoots').update({
                    'status': 'Completed', 
                    'output_image': public_url
                }).eq('id', photoshoot_id).execute()
                
                return jsonify({"message": "Image upscaled successfully", "url": public_url})

            except Exception as e:
                print(f"Upscale error: {e}")
                supabase.table('photoshoots').update({'status': 'Failed'}).eq('id', photoshoot_id).execute()
                return jsonify({"error": str(e)}), 500

                
        # Update the task with final status
        if update_data: # Ensure there's data to update before executing
            res = supabase.table('photoshoots').update(update_data).eq('id', photoshoot_id).execute()
            return jsonify({"photoshoot": res.data[0] if res.data else {}})
        
        return jsonify({"message": "No updates"})
    except Exception as e:
        print(f"Error updating photoshoot: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/photoshoots/<photoshoot_id>', methods=['DELETE'])
def delete_photoshoot(photoshoot_id):
    """Delete a photoshoot task"""
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        supabase.table('photoshoots').delete().eq('id', photoshoot_id).execute()
        return jsonify({"message": "Deleted successfully"})
    except Exception as e:
        print(f"Error deleting photoshoot: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/projects/<project_id>', methods=['DELETE'])
def delete_project(project_id):
    """Delete a project and all associated data"""
    if not supabase: return jsonify({"error": "Supabase not configured"}), 500
    
    try:
        # Delete the project (cascading should handle related data if configured in DB, 
        # otherwise we might need to delete related rows first. Assuming cascade for now or simple delete)
        supabase.table('projects').delete().eq('id', project_id).execute()
        return jsonify({"message": "Project deleted successfully"})
    except Exception as e:
        print(f"Error deleting project: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 3000))
    app.run(host='0.0.0.0', port=port, debug=True, use_reloader=False)
